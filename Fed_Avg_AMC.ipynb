{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-eqHSTvLGA8",
        "outputId": "76171332-ec1d-4f82-9d06-079e35c4e76c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'DW-FedAvg'...\n",
            "remote: Enumerating objects: 224, done.\u001b[K\n",
            "remote: Total 224 (delta 0), reused 0 (delta 0), pack-reused 224\u001b[K\n",
            "Receiving objects: 100% (224/224), 7.66 MiB | 8.42 MiB/s, done.\n",
            "Resolving deltas: 100% (74/74), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/officialarijit/DW-FedAvg.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTdPfdZ5pXlx"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5Ou2YUzpeCZ",
        "outputId": "76487f07-2794-455f-ece4-10a3daa1a5e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/DW-FedAvg\n"
          ]
        }
      ],
      "source": [
        "cd /content/DW-FedAvg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BR_YgmtYEEnY"
      },
      "source": [
        "# Federated Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMU7epISoL6r"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_C_wCu3ppAGW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import os\n",
        "# from imutils import paths\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "from sklearn import preprocessing\n",
        "\n",
        "from federated_utils import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6z5H7OTzpGCC"
      },
      "outputs": [],
      "source": [
        "#declear path to your data\n",
        "drebin_data_path = '/content/drive/MyDrive/drebin.csv'\n",
        "malgenome_data_path = '/content/drive/MyDrive/malgenome.csv'\n",
        "kronodroid_data_path = '/content/drive/MyDrive/kronodroid.csv'\n",
        "TUANDROMD_data_path='/content/drive/MyDrive/TUANDROMD.csv'\n",
        "\n",
        "\n",
        "\n",
        "Drebin_data = pd.read_csv(drebin_data_path, header = None)\n",
        "\n",
        "Malgenome_data = pd.read_csv(malgenome_data_path)\n",
        "\n",
        "Tuandromd_data=pd.read_csv(TUANDROMD_data_path)\n",
        "\n",
        "kronodroid_data=pd.read_csv(kronodroid_data_path)\n",
        "Kronodroid_data = kronodroid_data.iloc[:,range(1,kronodroid_data.shape[1])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5j8142cZGISd"
      },
      "outputs": [],
      "source": [
        "from keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers.legacy import SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1Xk6OjNrlRa",
        "outputId": "bb2e4a3b-b776-412f-e038-51ec09d83287"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===================================================================================================\n",
            "Working with: Drebin\n",
            "===================================================================================================\n",
            "---------------------------------------------\n",
            "No. of Clients: 5\n",
            "No. of Rounds: 10\n",
            "---------------------------------------------\n",
            "|=======================|\n",
            "|   DWFedAvg  2023      |\n",
            "|=======================|\n",
            "94/94 [==============================] - 0s 1ms/step\n",
            "comm_round: 0 | global_acc: 95.545% | global_loss: 0.6463699340820312 | global_f1: 0.9391462306993642 | global_precision: 0.9460201280878316 | global_recall: 0.9323715058611362 | global_auc: 0.9874002310551184| flobal_FPR: 0.06762849413886383 \n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 1 | global_acc: 96.144% | global_loss: 0.6029056906700134 | global_f1: 0.9469350411710887 | global_precision: 0.9610027855153204 | global_recall: 0.933273219116321 | global_auc: 0.9900431673259763| flobal_FPR: 0.06672678088367899 \n",
            "94/94 [==============================] - 0s 1ms/step\n",
            "comm_round: 2 | global_acc: 96.376% | global_loss: 0.592698872089386 | global_f1: 0.9501600365797896 | global_precision: 0.963821892393321 | global_recall: 0.9368800721370604 | global_auc: 0.9911784998131522| flobal_FPR: 0.06311992786293959 \n",
            "94/94 [==============================] - 0s 1ms/step\n",
            "comm_round: 3 | global_acc: 96.443% | global_loss: 0.5884047150611877 | global_f1: 0.9512083903328773 | global_precision: 0.9621771217712177 | global_recall: 0.9404869251577999 | global_auc: 0.9918845807033363| flobal_FPR: 0.059513074842200184 \n",
            "94/94 [==============================] - 0s 1ms/step\n",
            "comm_round: 4 | global_acc: 96.576% | global_loss: 0.5854157209396362 | global_f1: 0.9529465509365007 | global_precision: 0.9657407407407408 | global_recall: 0.9404869251577999 | global_auc: 0.9922653990449152| flobal_FPR: 0.059513074842200184 \n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 5 | global_acc: 96.676% | global_loss: 0.5837160348892212 | global_f1: 0.954337899543379 | global_precision: 0.9666975023126735 | global_recall: 0.9422903516681695 | global_auc: 0.9925916112651954| flobal_FPR: 0.057709648331830475 \n",
            "94/94 [==============================] - 0s 1ms/step\n",
            "comm_round: 6 | global_acc: 96.875% | global_loss: 0.5824225544929504 | global_f1: 0.9571948998178506 | global_precision: 0.9668813247470102 | global_recall: 0.9477006311992786 | global_auc: 0.992794841003594| flobal_FPR: 0.05229936880072137 \n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 7 | global_acc: 96.842% | global_loss: 0.5813363194465637 | global_f1: 0.9566803465572277 | global_precision: 0.9677121771217713 | global_recall: 0.9458972046889089 | global_auc: 0.9929325433964343| flobal_FPR: 0.05410279531109107 \n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 8 | global_acc: 96.908% | global_loss: 0.5807695388793945 | global_f1: 0.9576695493855257 | global_precision: 0.9669117647058824 | global_recall: 0.9486023444544635 | global_auc: 0.9930659722667382| flobal_FPR: 0.05139765554553652 \n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 9 | global_acc: 96.908% | global_loss: 0.5801169276237488 | global_f1: 0.9576695493855257 | global_precision: 0.9669117647058824 | global_recall: 0.9486023444544635 | global_auc: 0.9931604645983769| flobal_FPR: 0.05139765554553652 \n",
            "---------------------------------------------\n",
            "No. of Clients: 10\n",
            "No. of Rounds: 10\n",
            "---------------------------------------------\n",
            "|=======================|\n",
            "|   DWFedAvg  2023      |\n",
            "|=======================|\n",
            "94/94 [==============================] - 0s 1ms/step\n",
            "comm_round: 0 | global_acc: 93.318% | global_loss: 0.6727936267852783 | global_f1: 0.9063809967396367 | global_precision: 0.9373795761078998 | global_recall: 0.8773669972948602 | global_auc: 0.9783778753090587| flobal_FPR: 0.12263300270513977 \n",
            "94/94 [==============================] - 0s 1ms/step\n",
            "comm_round: 1 | global_acc: 94.415% | global_loss: 0.6361175775527954 | global_f1: 0.922723091076357 | global_precision: 0.9417840375586854 | global_recall: 0.9044183949504058 | global_auc: 0.9839386778006174| flobal_FPR: 0.09558160504959423 \n",
            "94/94 [==============================] - 0s 1ms/step\n",
            "comm_round: 2 | global_acc: 94.681% | global_loss: 0.6221708059310913 | global_f1: 0.926873857404022 | global_precision: 0.9397590361445783 | global_recall: 0.9143372407574392 | global_auc: 0.9858570145836331| flobal_FPR: 0.08566275924256087 \n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 3 | global_acc: 95.047% | global_loss: 0.6146444082260132 | global_f1: 0.9321184510250569 | global_precision: 0.9419889502762431 | global_recall: 0.9224526600541028 | global_auc: 0.9869424893078841| flobal_FPR: 0.0775473399458972 \n",
            "94/94 [==============================] - 0s 1ms/step\n",
            "comm_round: 4 | global_acc: 95.146% | global_loss: 0.609862744808197 | global_f1: 0.9335759781619655 | global_precision: 0.9421487603305785 | global_recall: 0.9251577998196574 | global_auc: 0.987685607393384| flobal_FPR: 0.07484220018034266 \n",
            "94/94 [==============================] - 0s 1ms/step\n",
            "comm_round: 5 | global_acc: 95.312% | global_loss: 0.6065902709960938 | global_f1: 0.9358799454297408 | global_precision: 0.9440366972477064 | global_recall: 0.9278629395852119 | global_auc: 0.9881922572318684| flobal_FPR: 0.0721370604147881 \n",
            "94/94 [==============================] - 0s 1ms/step\n",
            "comm_round: 6 | global_acc: 95.312% | global_loss: 0.6039826273918152 | global_f1: 0.9358799454297408 | global_precision: 0.9440366972477064 | global_recall: 0.9278629395852119 | global_auc: 0.9885574059908139| flobal_FPR: 0.0721370604147881 \n",
            "94/94 [==============================] - 0s 1ms/step\n",
            "comm_round: 7 | global_acc: 95.445% | global_loss: 0.6018272042274475 | global_f1: 0.9376989540700319 | global_precision: 0.9458715596330275 | global_recall: 0.9296663660955816 | global_auc: 0.9888271127464456| flobal_FPR: 0.0703336339044184 \n",
            "94/94 [==============================] - 0s 1ms/step\n",
            "comm_round: 8 | global_acc: 95.479% | global_loss: 0.6003249883651733 | global_f1: 0.9381818181818181 | global_precision: 0.9459211732355637 | global_recall: 0.9305680793507665 | global_auc: 0.9890631061576238| flobal_FPR: 0.06943192064923355 \n",
            "94/94 [==============================] - 0s 1ms/step\n",
            "comm_round: 9 | global_acc: 95.512% | global_loss: 0.5990857481956482 | global_f1: 0.9386642435256702 | global_precision: 0.9459706959706959 | global_recall: 0.9314697926059513 | global_auc: 0.9893062221063623| flobal_FPR: 0.06853020739404869 \n",
            "---------------------------------------------\n",
            "No. of Clients: 15\n",
            "No. of Rounds: 10\n",
            "---------------------------------------------\n",
            "|=======================|\n",
            "|   DWFedAvg  2023      |\n",
            "|=======================|\n",
            "94/94 [==============================] - 0s 1ms/step\n",
            "comm_round: 0 | global_acc: 92.487% | global_loss: 0.6665503978729248 | global_f1: 0.8964252978918423 | global_precision: 0.9114631873252563 | global_recall: 0.8818755635707844 | global_auc: 0.9750426283873006| flobal_FPR: 0.11812443642921551 \n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 1 | global_acc: 93.816% | global_loss: 0.6414099335670471 | global_f1: 0.9152233363719234 | global_precision: 0.9253456221198156 | global_recall: 0.9053201082055906 | global_auc: 0.9815298355975881| flobal_FPR: 0.09467989179440937 \n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 2 | global_acc: 94.116% | global_loss: 0.6303186416625977 | global_f1: 0.919582008178101 | global_precision: 0.9267399267399268 | global_recall: 0.9125338142470695 | global_auc: 0.9836138900878494| flobal_FPR: 0.08746618575293057 \n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 3 | global_acc: 94.182% | global_loss: 0.6234484910964966 | global_f1: 0.920418371987267 | global_precision: 0.9284403669724771 | global_recall: 0.9125338142470695 | global_auc: 0.9846656514676463| flobal_FPR: 0.08746618575293057 \n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 4 | global_acc: 94.448% | global_loss: 0.6191688179969788 | global_f1: 0.923987255348202 | global_precision: 0.9329044117647058 | global_recall: 0.915238954012624 | global_auc: 0.9854429577334376| flobal_FPR: 0.08476104598737602 \n",
            "94/94 [==============================] - 0s 1ms/step\n",
            "comm_round: 5 | global_acc: 94.548% | global_loss: 0.615893542766571 | global_f1: 0.9255222524977293 | global_precision: 0.9322964318389753 | global_recall: 0.9188458070333634 | global_auc: 0.9859951918123107| flobal_FPR: 0.0811541929666366 \n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 6 | global_acc: 94.548% | global_loss: 0.6134589314460754 | global_f1: 0.9254545454545455 | global_precision: 0.9330889092575618 | global_recall: 0.9179440937781785 | global_auc: 0.9864215943942779| flobal_FPR: 0.08205590622182146 \n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 7 | global_acc: 94.581% | global_loss: 0.6113975644111633 | global_f1: 0.92580791989076 | global_precision: 0.9347426470588235 | global_recall: 0.9170423805229937 | global_auc: 0.9867957650341335| flobal_FPR: 0.0829576194770063 \n",
            "94/94 [==============================] - 0s 1ms/step\n",
            "comm_round: 8 | global_acc: 94.747% | global_loss: 0.6096894145011902 | global_f1: 0.9278538812785389 | global_precision: 0.9398704902867715 | global_recall: 0.9161406672678089 | global_auc: 0.987140970687909| flobal_FPR: 0.08385933273219116 \n",
            "94/94 [==============================] - 0s 1ms/step\n",
            "comm_round: 9 | global_acc: 94.747% | global_loss: 0.6083045601844788 | global_f1: 0.9277879341864717 | global_precision: 0.9406858202038925 | global_recall: 0.915238954012624 | global_auc: 0.9873802879499485| flobal_FPR: 0.08476104598737602 \n",
            "---------------------------------------------\n",
            "No. of Clients: 5\n",
            "No. of Rounds: 20\n",
            "---------------------------------------------\n",
            "|=======================|\n",
            "|   DWFedAvg  2023      |\n",
            "|=======================|\n",
            "94/94 [==============================] - 0s 1ms/step\n",
            "comm_round: 0 | global_acc: 95.745% | global_loss: 0.6144077777862549 | global_f1: 0.9418710263396912 | global_precision: 0.9487648673376029 | global_recall: 0.9350766456266907 | global_auc: 0.9918993006142951| flobal_FPR: 0.06492335437330929 \n",
            "94/94 [==============================] - 0s 1ms/step\n",
            "comm_round: 1 | global_acc: 96.310% | global_loss: 0.5837990045547485 | global_f1: 0.9493382017343678 | global_precision: 0.9611829944547134 | global_recall: 0.9377817853922452 | global_auc: 0.9931305499406218| flobal_FPR: 0.062218214607754736 \n",
            "94/94 [==============================] - 0s 1ms/step\n",
            "comm_round: 2 | global_acc: 96.809% | global_loss: 0.5788969397544861 | global_f1: 0.9564032697547684 | global_precision: 0.9634034766697164 | global_recall: 0.9495040577096483 | global_auc: 0.9937649306193617| flobal_FPR: 0.05049594229035167 \n",
            "94/94 [==============================] - 0s 1ms/step\n",
            "comm_round: 3 | global_acc: 97.174% | global_loss: 0.5767131447792053 | global_f1: 0.9614512471655328 | global_precision: 0.9671532846715328 | global_recall: 0.9558160504959423 | global_auc: 0.9941428999459162| flobal_FPR: 0.04418394950405771 \n",
            "94/94 [==============================] - 0s 1ms/step\n",
            "comm_round: 4 | global_acc: 97.407% | global_loss: 0.5754209756851196 | global_f1: 0.9646098003629765 | global_precision: 0.9707762557077626 | global_recall: 0.9585211902614968 | global_auc: 0.994391239088866| flobal_FPR: 0.04147880973850315 \n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 5 | global_acc: 97.540% | global_loss: 0.5743892192840576 | global_f1: 0.9664246823956443 | global_precision: 0.9726027397260274 | global_recall: 0.9603246167718665 | global_auc: 0.9945721515429078| flobal_FPR: 0.03967538322813345 \n",
            "94/94 [==============================] - 0s 1ms/step\n",
            "comm_round: 6 | global_acc: 97.540% | global_loss: 0.5738191604614258 | global_f1: 0.9664246823956443 | global_precision: 0.9726027397260274 | global_recall: 0.9603246167718665 | global_auc: 0.9947150771299592| flobal_FPR: 0.03967538322813345 \n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 7 | global_acc: 97.606% | global_loss: 0.5731183290481567 | global_f1: 0.9673321234119783 | global_precision: 0.9735159817351599 | global_recall: 0.9612263300270514 | global_auc: 0.9947749064454691| flobal_FPR: 0.0387736699729486 \n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 8 | global_acc: 97.640% | global_loss: 0.572834312915802 | global_f1: 0.9678004535147392 | global_precision: 0.9735401459854015 | global_recall: 0.9621280432822362 | global_auc: 0.9948371099401658| flobal_FPR: 0.03787195671776375 \n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 9 | global_acc: 97.673% | global_loss: 0.5725157260894775 | global_f1: 0.9682683590208523 | global_precision: 0.9735642661804923 | global_recall: 0.9630297565374211 | global_auc: 0.9949273287492681| flobal_FPR: 0.0369702434625789 \n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 10 | global_acc: 97.673% | global_loss: 0.5722222328186035 | global_f1: 0.9682683590208523 | global_precision: 0.9735642661804923 | global_recall: 0.9630297565374211 | global_auc: 0.9949733878254939| flobal_FPR: 0.0369702434625789 \n",
            "94/94 [==============================] - 0s 1ms/step\n",
            "comm_round: 11 | global_acc: 97.640% | global_loss: 0.5720648169517517 | global_f1: 0.9678004535147392 | global_precision: 0.9735401459854015 | global_recall: 0.9621280432822362 | global_auc: 0.99501944690172| flobal_FPR: 0.03787195671776375 \n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 12 | global_acc: 97.640% | global_loss: 0.5717194676399231 | global_f1: 0.9678004535147392 | global_precision: 0.9735401459854015 | global_recall: 0.9621280432822362 | global_auc: 0.9950479370519627| flobal_FPR: 0.03787195671776375 \n",
            "94/94 [==============================] - 0s 1ms/step\n",
            "comm_round: 13 | global_acc: 97.640% | global_loss: 0.5715541243553162 | global_f1: 0.9678004535147392 | global_precision: 0.9735401459854015 | global_recall: 0.9621280432822362 | global_auc: 0.995050786066987| flobal_FPR: 0.03787195671776375 \n",
            "94/94 [==============================] - 0s 1ms/step\n",
            "comm_round: 14 | global_acc: 97.640% | global_loss: 0.5714993476867676 | global_f1: 0.9678004535147392 | global_precision: 0.9735401459854015 | global_recall: 0.9621280432822362 | global_auc: 0.9951072915316351| flobal_FPR: 0.03787195671776375 \n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 15 | global_acc: 97.640% | global_loss: 0.5712993741035461 | global_f1: 0.9678004535147392 | global_precision: 0.9735401459854015 | global_recall: 0.9621280432822362 | global_auc: 0.9951476525778127| flobal_FPR: 0.03787195671776375 \n",
            "94/94 [==============================] - 0s 1ms/step\n",
            "comm_round: 16 | global_acc: 97.673% | global_loss: 0.5710958242416382 | global_f1: 0.968239564428312 | global_precision: 0.9744292237442922 | global_recall: 0.9621280432822362 | global_auc: 0.9951799414147544| flobal_FPR: 0.03787195671776375 \n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 17 | global_acc: 97.673% | global_loss: 0.5709666013717651 | global_f1: 0.968239564428312 | global_precision: 0.9744292237442922 | global_recall: 0.9621280432822362 | global_auc: 0.9951899129673394| flobal_FPR: 0.03787195671776375 \n",
            "94/94 [==============================] - 0s 1ms/step\n",
            "comm_round: 18 | global_acc: 97.673% | global_loss: 0.5708284378051758 | global_f1: 0.968239564428312 | global_precision: 0.9744292237442922 | global_recall: 0.9621280432822362 | global_auc: 0.9951998845199244| flobal_FPR: 0.03787195671776375 \n",
            "94/94 [==============================] - 0s 1ms/step\n",
            "comm_round: 19 | global_acc: 97.673% | global_loss: 0.570717990398407 | global_f1: 0.968239564428312 | global_precision: 0.9744292237442922 | global_recall: 0.9621280432822362 | global_auc: 0.9952411952377764| flobal_FPR: 0.03787195671776375 \n",
            "---------------------------------------------\n",
            "No. of Clients: 10\n",
            "No. of Rounds: 20\n",
            "---------------------------------------------\n",
            "|=======================|\n",
            "|   DWFedAvg  2023      |\n",
            "|=======================|\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 0 | global_acc: 95.047% | global_loss: 0.6301182508468628 | global_f1: 0.932487539646579 | global_precision: 0.9371584699453552 | global_recall: 0.9278629395852119 | global_auc: 0.9845711591360077| flobal_FPR: 0.0721370604147881 \n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 1 | global_acc: 95.711% | global_loss: 0.5999365448951721 | global_f1: 0.9412300683371299 | global_precision: 0.9511970534069981 | global_recall: 0.9314697926059513 | global_auc: 0.9879111544161394| flobal_FPR: 0.06853020739404869 \n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 2 | global_acc: 96.210% | global_loss: 0.5923970937728882 | global_f1: 0.947992700729927 | global_precision: 0.9593721144967683 | global_recall: 0.9368800721370604 | global_auc: 0.9896101170422856| flobal_FPR: 0.06311992786293959 \n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 3 | global_acc: 96.243% | global_loss: 0.5888915061950684 | global_f1: 0.9485193621867881 | global_precision: 0.9585635359116023 | global_recall: 0.9386834986474302 | global_auc: 0.990557414537859| flobal_FPR: 0.061316501352569885 \n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 4 | global_acc: 96.443% | global_loss: 0.5862393975257874 | global_f1: 0.9512083903328773 | global_precision: 0.9621771217712177 | global_recall: 0.9404869251577999 | global_auc: 0.9910844823173508| flobal_FPR: 0.059513074842200184 \n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 5 | global_acc: 96.376% | global_loss: 0.584858775138855 | global_f1: 0.9504320145520693 | global_precision: 0.9587155963302753 | global_recall: 0.9422903516681695 | global_auc: 0.9914448827179225| flobal_FPR: 0.057709648331830475 \n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 6 | global_acc: 96.410% | global_loss: 0.5838544964790344 | global_f1: 0.9509090909090908 | global_precision: 0.9587534372135655 | global_recall: 0.9431920649233544 | global_auc: 0.9917012940701075| flobal_FPR: 0.056807935076645624 \n",
            "94/94 [==============================] - 0s 1ms/step\n",
            "comm_round: 7 | global_acc: 96.509% | global_loss: 0.582761824131012 | global_f1: 0.9522075557578515 | global_precision: 0.9613970588235294 | global_recall: 0.9431920649233544 | global_auc: 0.9919482087055452| flobal_FPR: 0.056807935076645624 \n",
            "94/94 [==============================] - 0s 1ms/step\n",
            "comm_round: 8 | global_acc: 96.543% | global_loss: 0.5819995999336243 | global_f1: 0.95264116575592 | global_precision: 0.9622815087396505 | global_recall: 0.9431920649233544 | global_auc: 0.9921495391005943| flobal_FPR: 0.056807935076645624 \n",
            "94/94 [==============================] - 0s 1ms/step\n",
            "comm_round: 9 | global_acc: 96.576% | global_loss: 0.5815449357032776 | global_f1: 0.9531178880291307 | global_precision: 0.9623161764705882 | global_recall: 0.9440937781785392 | global_auc: 0.9923214296737259| flobal_FPR: 0.05590622182146077 \n",
            "94/94 [==============================] - 0s 1ms/step\n",
            "comm_round: 10 | global_acc: 96.642% | global_loss: 0.5810374617576599 | global_f1: 0.9540282203004097 | global_precision: 0.9632352941176471 | global_recall: 0.9449954914337241 | global_auc: 0.9924420379764207| flobal_FPR: 0.05500450856627592 \n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 11 | global_acc: 96.908% | global_loss: 0.580623984336853 | global_f1: 0.9578231292517009 | global_precision: 0.9635036496350365 | global_recall: 0.9522091974752029 | global_auc: 0.9925631211149525| flobal_FPR: 0.047790802524797116 \n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 12 | global_acc: 96.941% | global_loss: 0.5801975727081299 | global_f1: 0.9582577132486388 | global_precision: 0.9643835616438357 | global_recall: 0.9522091974752029 | global_auc: 0.9926775565517612| flobal_FPR: 0.047790802524797116 \n",
            "94/94 [==============================] - 0s 1ms/step\n",
            "comm_round: 13 | global_acc: 96.941% | global_loss: 0.5798882246017456 | global_f1: 0.9582577132486388 | global_precision: 0.9643835616438357 | global_recall: 0.9522091974752029 | global_auc: 0.9927411845539701| flobal_FPR: 0.047790802524797116 \n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 14 | global_acc: 96.941% | global_loss: 0.5797392725944519 | global_f1: 0.958295557570263 | global_precision: 0.9635369188696444 | global_recall: 0.9531109107303878 | global_auc: 0.9928005390336427| flobal_FPR: 0.046889089269612265 \n",
            "94/94 [==============================] - 0s 1ms/step\n",
            "comm_round: 15 | global_acc: 96.941% | global_loss: 0.5793901085853577 | global_f1: 0.958295557570263 | global_precision: 0.9635369188696444 | global_recall: 0.9531109107303878 | global_auc: 0.9928446987665189| flobal_FPR: 0.046889089269612265 \n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 16 | global_acc: 96.975% | global_loss: 0.5791208148002625 | global_f1: 0.9587675577707295 | global_precision: 0.9635701275045537 | global_recall: 0.9540126239855726 | global_auc: 0.9929102261120776| flobal_FPR: 0.045987376014427414 \n",
            "94/94 [==============================] - 0s 3ms/step\n",
            "comm_round: 17 | global_acc: 97.008% | global_loss: 0.5787081122398376 | global_f1: 0.9591280653950954 | global_precision: 0.9661482159194876 | global_recall: 0.9522091974752029 | global_auc: 0.9929638825617013| flobal_FPR: 0.047790802524797116 \n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 18 | global_acc: 97.041% | global_loss: 0.5785165429115295 | global_f1: 0.9596005447117567 | global_precision: 0.9661791590493601 | global_recall: 0.9531109107303878 | global_auc: 0.993020862862187| flobal_FPR: 0.046889089269612265 \n",
            "94/94 [==============================] - 0s 1ms/step\n",
            "comm_round: 19 | global_acc: 97.074% | global_loss: 0.5783489942550659 | global_f1: 0.9600725952813066 | global_precision: 0.9662100456621004 | global_recall: 0.9540126239855726 | global_auc: 0.9930664471025755| flobal_FPR: 0.045987376014427414 \n",
            "---------------------------------------------\n",
            "No. of Clients: 15\n",
            "No. of Rounds: 20\n",
            "---------------------------------------------\n",
            "|=======================|\n",
            "|   DWFedAvg  2023      |\n",
            "|=======================|\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 0 | global_acc: 93.617% | global_loss: 0.6497082710266113 | global_f1: 0.9122486288848264 | global_precision: 0.9249304911955515 | global_recall: 0.8999098286744815 | global_auc: 0.979573511947582| flobal_FPR: 0.10009017132551848 \n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 1 | global_acc: 94.481% | global_loss: 0.6188485026359558 | global_f1: 0.9242009132420091 | global_precision: 0.9361702127659575 | global_recall: 0.9125338142470695 | global_auc: 0.9845835048677796| flobal_FPR: 0.08746618575293057 \n",
            "94/94 [==============================] - 0s 1ms/step\n",
            "comm_round: 2 | global_acc: 94.914% | global_loss: 0.6083246469497681 | global_f1: 0.9306751246035343 | global_precision: 0.9353369763205829 | global_recall: 0.9260595130748422 | global_auc: 0.9864021261249454| flobal_FPR: 0.0739404869251578 \n",
            "94/94 [==============================] - 0s 1ms/step\n",
            "comm_round: 3 | global_acc: 95.346% | global_loss: 0.6024973392486572 | global_f1: 0.9364791288566243 | global_precision: 0.9424657534246575 | global_recall: 0.9305680793507665 | global_auc: 0.9875046949393421| flobal_FPR: 0.06943192064923355 \n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 4 | global_acc: 95.578% | global_loss: 0.5991061329841614 | global_f1: 0.939791760977818 | global_precision: 0.9436363636363636 | global_recall: 0.9359783588818755 | global_auc: 0.9882454388456551| flobal_FPR: 0.06402164111812443 \n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 5 | global_acc: 95.578% | global_loss: 0.5967528223991394 | global_f1: 0.9399005874378671 | global_precision: 0.9420289855072463 | global_recall: 0.9377817853922452 | global_auc: 0.9889880820953176| flobal_FPR: 0.062218214607754736 \n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 6 | global_acc: 95.745% | global_loss: 0.5946997404098511 | global_f1: 0.9421338155515372 | global_precision: 0.9446962828649139 | global_recall: 0.939585211902615 | global_auc: 0.9894353774541297| flobal_FPR: 0.060414788097385035 \n",
            "94/94 [==============================] - 0s 1ms/step\n",
            "comm_round: 7 | global_acc: 95.844% | global_loss: 0.5932871103286743 | global_f1: 0.9433620299048482 | global_precision: 0.9480874316939891 | global_recall: 0.9386834986474302 | global_auc: 0.9898299660349925| flobal_FPR: 0.061316501352569885 \n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 8 | global_acc: 95.944% | global_loss: 0.5922983884811401 | global_f1: 0.9447963800904977 | global_precision: 0.9482288828337875 | global_recall: 0.9413886384129847 | global_auc: 0.9901257887616803| flobal_FPR: 0.058611361587015326 \n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 9 | global_acc: 95.878% | global_loss: 0.5912471413612366 | global_f1: 0.9438914027149321 | global_precision: 0.9473206176203451 | global_recall: 0.9404869251577999 | global_auc: 0.990375077576305| flobal_FPR: 0.059513074842200184 \n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 10 | global_acc: 95.878% | global_loss: 0.5903794169425964 | global_f1: 0.9438914027149321 | global_precision: 0.9473206176203451 | global_recall: 0.9404869251577999 | global_auc: 0.9905911278823127| flobal_FPR: 0.059513074842200184 \n",
            "94/94 [==============================] - 0s 1ms/step\n",
            "comm_round: 11 | global_acc: 95.944% | global_loss: 0.5896652936935425 | global_f1: 0.9447963800904977 | global_precision: 0.9482288828337875 | global_recall: 0.9413886384129847 | global_auc: 0.9907981563074107| flobal_FPR: 0.058611361587015326 \n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 12 | global_acc: 95.977% | global_loss: 0.5890471935272217 | global_f1: 0.945273631840796 | global_precision: 0.9482758620689655 | global_recall: 0.9422903516681695 | global_auc: 0.9909301606702022| flobal_FPR: 0.057709648331830475 \n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 13 | global_acc: 95.944% | global_loss: 0.5885290503501892 | global_f1: 0.9447963800904977 | global_precision: 0.9482288828337875 | global_recall: 0.9413886384129847 | global_auc: 0.9910631147046686| flobal_FPR: 0.058611361587015326 \n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 14 | global_acc: 95.944% | global_loss: 0.5881024599075317 | global_f1: 0.9447963800904977 | global_precision: 0.9482288828337875 | global_recall: 0.9413886384129847 | global_auc: 0.9911751759622903| flobal_FPR: 0.058611361587015326 \n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 15 | global_acc: 95.977% | global_loss: 0.587648332118988 | global_f1: 0.9452240832956089 | global_precision: 0.9490909090909091 | global_recall: 0.9413886384129847 | global_auc: 0.9912858127124| flobal_FPR: 0.058611361587015326 \n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 16 | global_acc: 95.977% | global_loss: 0.5872751474380493 | global_f1: 0.9452240832956089 | global_precision: 0.9490909090909091 | global_recall: 0.9413886384129847 | global_auc: 0.9913983488058591| flobal_FPR: 0.058611361587015326 \n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 17 | global_acc: 96.044% | global_loss: 0.5869024395942688 | global_f1: 0.946129470348574 | global_precision: 0.95 | global_recall: 0.9422903516681695 | global_auc: 0.9914904669583109| flobal_FPR: 0.057709648331830475 \n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 18 | global_acc: 96.110% | global_loss: 0.5865399241447449 | global_f1: 0.946986859990938 | global_precision: 0.9517304189435337 | global_recall: 0.9422903516681695 | global_auc: 0.9915612174980806| flobal_FPR: 0.057709648331830475 \n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "comm_round: 19 | global_acc: 96.110% | global_loss: 0.5862542986869812 | global_f1: 0.946986859990938 | global_precision: 0.9517304189435337 | global_recall: 0.9422903516681695 | global_auc: 0.9916409899187604| flobal_FPR: 0.057709648331830475 \n",
            "===================================================================================================\n",
            "Working with: Malgenome\n",
            "===================================================================================================\n",
            "---------------------------------------------\n",
            "No. of Clients: 5\n",
            "No. of Rounds: 10\n",
            "---------------------------------------------\n",
            "|=======================|\n",
            "|   DWFedAvg  2023      |\n",
            "|=======================|\n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 0 | global_acc: 96.579% | global_loss: 0.6479014158248901 | global_f1: 0.9471544715447154 | global_precision: 1.0 | global_recall: 0.8996138996138996 | global_auc: 0.9941969343166949| flobal_FPR: 0.10038610038610038 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 1 | global_acc: 96.974% | global_loss: 0.6019136309623718 | global_f1: 0.9535353535353535 | global_precision: 1.0 | global_recall: 0.9111969111969112 | global_auc: 0.9969173621868233| flobal_FPR: 0.0888030888030888 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 2 | global_acc: 97.368% | global_loss: 0.5920145511627197 | global_f1: 0.9601593625498009 | global_precision: 0.9917695473251029 | global_recall: 0.9305019305019305 | global_auc: 0.9977034348291833| flobal_FPR: 0.0694980694980695 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 3 | global_acc: 97.895% | global_loss: 0.5875027179718018 | global_f1: 0.9683794466403162 | global_precision: 0.9919028340080972 | global_recall: 0.9459459459459459 | global_auc: 0.9982043634738246| flobal_FPR: 0.05405405405405406 \n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "comm_round: 4 | global_acc: 98.289% | global_loss: 0.5851645469665527 | global_f1: 0.9744597249508842 | global_precision: 0.992 | global_recall: 0.9575289575289575 | global_auc: 0.9985819866059388| flobal_FPR: 0.04247104247104247 \n",
            "24/24 [==============================] - 0s 3ms/step\n",
            "comm_round: 5 | global_acc: 98.289% | global_loss: 0.5833610892295837 | global_f1: 0.9744597249508842 | global_precision: 0.992 | global_recall: 0.9575289575289575 | global_auc: 0.9987669448747293| flobal_FPR: 0.04247104247104247 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 6 | global_acc: 98.421% | global_loss: 0.5823953747749329 | global_f1: 0.9764705882352941 | global_precision: 0.9920318725099602 | global_recall: 0.9613899613899614 | global_auc: 0.9988748371981905| flobal_FPR: 0.03861003861003861 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 7 | global_acc: 98.684% | global_loss: 0.5813963413238525 | global_f1: 0.98046875 | global_precision: 0.9920948616600791 | global_recall: 0.9691119691119691 | global_auc: 0.9989519031435199| flobal_FPR: 0.03088803088803089 \n",
            "24/24 [==============================] - 0s 3ms/step\n",
            "comm_round: 8 | global_acc: 98.684% | global_loss: 0.5806942582130432 | global_f1: 0.98046875 | global_precision: 0.9920948616600791 | global_recall: 0.9691119691119691 | global_auc: 0.9990597954669811| flobal_FPR: 0.03088803088803089 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 9 | global_acc: 98.684% | global_loss: 0.5799755454063416 | global_f1: 0.98046875 | global_precision: 0.9920948616600791 | global_recall: 0.9691119691119691 | global_auc: 0.9991060350341787| flobal_FPR: 0.03088803088803089 \n",
            "---------------------------------------------\n",
            "No. of Clients: 10\n",
            "No. of Rounds: 10\n",
            "---------------------------------------------\n",
            "|=======================|\n",
            "|   DWFedAvg  2023      |\n",
            "|=======================|\n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 0 | global_acc: 90.789% | global_loss: 0.684823751449585 | global_f1: 0.8451327433628317 | global_precision: 0.9896373056994818 | global_recall: 0.7374517374517374 | global_auc: 0.9864209804329565| flobal_FPR: 0.2625482625482625 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 1 | global_acc: 93.026% | global_loss: 0.6422373652458191 | global_f1: 0.8874734607218684 | global_precision: 0.9858490566037735 | global_recall: 0.806949806949807 | global_auc: 0.9920776208201358| flobal_FPR: 0.19305019305019305 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 2 | global_acc: 93.816% | global_loss: 0.6256998777389526 | global_f1: 0.9018789144050104 | global_precision: 0.9818181818181818 | global_recall: 0.833976833976834 | global_auc: 0.9933954484852687| flobal_FPR: 0.16602316602316602 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 3 | global_acc: 94.211% | global_loss: 0.617099940776825 | global_f1: 0.9087136929460581 | global_precision: 0.9820627802690582 | global_recall: 0.8455598455598455 | global_auc: 0.994150694749497| flobal_FPR: 0.15444015444015444 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 4 | global_acc: 95.263% | global_loss: 0.6115275621414185 | global_f1: 0.9265306122448981 | global_precision: 0.9826839826839827 | global_recall: 0.8764478764478765 | global_auc: 0.994713276150402| flobal_FPR: 0.12355212355212356 \n",
            "24/24 [==============================] - 0s 3ms/step\n",
            "comm_round: 5 | global_acc: 95.789% | global_loss: 0.6078436374664307 | global_f1: 0.9352226720647773 | global_precision: 0.9829787234042553 | global_recall: 0.8918918918918919 | global_auc: 0.9950138333371866| flobal_FPR: 0.10810810810810811 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 6 | global_acc: 96.184% | global_loss: 0.6050282120704651 | global_f1: 0.9416498993963781 | global_precision: 0.9831932773109243 | global_recall: 0.9034749034749034 | global_auc: 0.9954376960364985| flobal_FPR: 0.09652509652509653 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 7 | global_acc: 96.711% | global_loss: 0.6028574705123901 | global_f1: 0.9500998003992016 | global_precision: 0.9834710743801653 | global_recall: 0.918918918918919 | global_auc: 0.9955764147380913| flobal_FPR: 0.08108108108108109 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 8 | global_acc: 96.711% | global_loss: 0.6010642051696777 | global_f1: 0.9500998003992016 | global_precision: 0.9834710743801653 | global_recall: 0.918918918918919 | global_auc: 0.9957151334396844| flobal_FPR: 0.08108108108108109 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 9 | global_acc: 96.579% | global_loss: 0.599699854850769 | global_f1: 0.948207171314741 | global_precision: 0.9794238683127572 | global_recall: 0.918918918918919 | global_auc: 0.9958615587358103| flobal_FPR: 0.08108108108108109 \n",
            "---------------------------------------------\n",
            "No. of Clients: 15\n",
            "No. of Rounds: 10\n",
            "---------------------------------------------\n",
            "|=======================|\n",
            "|   DWFedAvg  2023      |\n",
            "|=======================|\n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 0 | global_acc: 83.684% | global_loss: 0.7149993777275085 | global_f1: 0.6868686868686867 | global_precision: 0.9927007299270073 | global_recall: 0.525096525096525 | global_auc: 0.9713314683374563| flobal_FPR: 0.4749034749034749 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 1 | global_acc: 90.789% | global_loss: 0.6771885752677917 | global_f1: 0.8444444444444443 | global_precision: 0.9947643979057592 | global_recall: 0.7335907335907336 | global_auc: 0.9884246950115213| flobal_FPR: 0.26640926640926643 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 2 | global_acc: 93.553% | global_loss: 0.6574952006340027 | global_f1: 0.8959660297239915 | global_precision: 0.9952830188679245 | global_recall: 0.8146718146718147 | global_auc: 0.9921778065490641| flobal_FPR: 0.18532818532818532 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 3 | global_acc: 95.000% | global_loss: 0.6453235745429993 | global_f1: 0.921487603305785 | global_precision: 0.9911111111111112 | global_recall: 0.861003861003861 | global_auc: 0.9934185682688677| flobal_FPR: 0.138996138996139 \n",
            "24/24 [==============================] - 0s 3ms/step\n",
            "comm_round: 4 | global_acc: 95.000% | global_loss: 0.6368921399116516 | global_f1: 0.9221311475409837 | global_precision: 0.982532751091703 | global_recall: 0.8687258687258688 | global_auc: 0.9941738145330961| flobal_FPR: 0.13127413127413126 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 5 | global_acc: 95.132% | global_loss: 0.6308284401893616 | global_f1: 0.9243353783231083 | global_precision: 0.9826086956521739 | global_recall: 0.8725868725868726 | global_auc: 0.9946824497722702| flobal_FPR: 0.1274131274131274 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 6 | global_acc: 95.526% | global_loss: 0.6262286305427551 | global_f1: 0.9308943089430894 | global_precision: 0.9828326180257511 | global_recall: 0.8841698841698842 | global_auc: 0.9950600729043843| flobal_FPR: 0.11583011583011583 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 7 | global_acc: 95.526% | global_loss: 0.622642457485199 | global_f1: 0.9308943089430894 | global_precision: 0.9828326180257511 | global_recall: 0.8841698841698842 | global_auc: 0.995360630091169| flobal_FPR: 0.11583011583011583 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 8 | global_acc: 95.658% | global_loss: 0.619627058506012 | global_f1: 0.9330628803245435 | global_precision: 0.9829059829059829 | global_recall: 0.888030888030888 | global_auc: 0.9955841213326244| flobal_FPR: 0.11196911196911197 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 9 | global_acc: 95.789% | global_loss: 0.6170601844787598 | global_f1: 0.9352226720647773 | global_precision: 0.9829787234042553 | global_recall: 0.8918918918918919 | global_auc: 0.9957613730068821| flobal_FPR: 0.10810810810810811 \n",
            "---------------------------------------------\n",
            "No. of Clients: 5\n",
            "No. of Rounds: 20\n",
            "---------------------------------------------\n",
            "|=======================|\n",
            "|   DWFedAvg  2023      |\n",
            "|=======================|\n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 0 | global_acc: 97.500% | global_loss: 0.6330543756484985 | global_f1: 0.9620758483033932 | global_precision: 0.9958677685950413 | global_recall: 0.9305019305019305 | global_auc: 0.9981581239066268| flobal_FPR: 0.0694980694980695 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 1 | global_acc: 98.158% | global_loss: 0.5884467959403992 | global_f1: 0.9725490196078432 | global_precision: 0.9880478087649402 | global_recall: 0.9575289575289575 | global_auc: 0.9988825437927233| flobal_FPR: 0.04247104247104247 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 2 | global_acc: 98.553% | global_loss: 0.5819309949874878 | global_f1: 0.9786407766990292 | global_precision: 0.984375 | global_recall: 0.972972972972973 | global_auc: 0.9991599811959093| flobal_FPR: 0.02702702702702703 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 3 | global_acc: 98.684% | global_loss: 0.578888475894928 | global_f1: 0.9806201550387598 | global_precision: 0.9844357976653697 | global_recall: 0.9768339768339769 | global_auc: 0.9993449394646999| flobal_FPR: 0.023166023166023165 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 4 | global_acc: 98.684% | global_loss: 0.5772374272346497 | global_f1: 0.9806201550387598 | global_precision: 0.9844357976653697 | global_recall: 0.9768339768339769 | global_auc: 0.999452831788161| flobal_FPR: 0.023166023166023165 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 5 | global_acc: 98.816% | global_loss: 0.5760917067527771 | global_f1: 0.9825918762088974 | global_precision: 0.9844961240310077 | global_recall: 0.9806949806949807 | global_auc: 0.9994913647608259| flobal_FPR: 0.019305019305019305 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 6 | global_acc: 98.947% | global_loss: 0.5755907893180847 | global_f1: 0.9846153846153846 | global_precision: 0.9808429118773946 | global_recall: 0.9884169884169884 | global_auc: 0.9995530175170895| flobal_FPR: 0.011583011583011582 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 7 | global_acc: 99.079% | global_loss: 0.5748167634010315 | global_f1: 0.9865125240847784 | global_precision: 0.9846153846153847 | global_recall: 0.9884169884169884 | global_auc: 0.9995838438952211| flobal_FPR: 0.011583011583011582 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 8 | global_acc: 99.079% | global_loss: 0.5744081139564514 | global_f1: 0.9865125240847784 | global_precision: 0.9846153846153847 | global_recall: 0.9884169884169884 | global_auc: 0.9996146702733528| flobal_FPR: 0.011583011583011582 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 9 | global_acc: 99.079% | global_loss: 0.5740594267845154 | global_f1: 0.9865125240847784 | global_precision: 0.9846153846153847 | global_recall: 0.9884169884169884 | global_auc: 0.9996300834624188| flobal_FPR: 0.011583011583011582 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 10 | global_acc: 99.079% | global_loss: 0.5738000273704529 | global_f1: 0.9865125240847784 | global_precision: 0.9846153846153847 | global_recall: 0.9884169884169884 | global_auc: 0.9996532032460176| flobal_FPR: 0.011583011583011582 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 11 | global_acc: 99.079% | global_loss: 0.5734620094299316 | global_f1: 0.9865125240847784 | global_precision: 0.9846153846153847 | global_recall: 0.9884169884169884 | global_auc: 0.9996609098405506| flobal_FPR: 0.011583011583011582 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 12 | global_acc: 99.079% | global_loss: 0.5730908513069153 | global_f1: 0.9865125240847784 | global_precision: 0.9846153846153847 | global_recall: 0.9884169884169884 | global_auc: 0.9996609098405506| flobal_FPR: 0.011583011583011582 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 13 | global_acc: 98.947% | global_loss: 0.5731478929519653 | global_f1: 0.9846153846153846 | global_precision: 0.9808429118773946 | global_recall: 0.9884169884169884 | global_auc: 0.9996609098405507| flobal_FPR: 0.011583011583011582 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 14 | global_acc: 99.079% | global_loss: 0.5730114579200745 | global_f1: 0.9865125240847784 | global_precision: 0.9846153846153847 | global_recall: 0.9884169884169884 | global_auc: 0.9996763230296165| flobal_FPR: 0.011583011583011582 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 15 | global_acc: 99.079% | global_loss: 0.5727928280830383 | global_f1: 0.9865125240847784 | global_precision: 0.9846153846153847 | global_recall: 0.9884169884169884 | global_auc: 0.9996840296241495| flobal_FPR: 0.011583011583011582 \n",
            "24/24 [==============================] - 0s 3ms/step\n",
            "comm_round: 16 | global_acc: 99.079% | global_loss: 0.5726931691169739 | global_f1: 0.9865125240847784 | global_precision: 0.9846153846153847 | global_recall: 0.9884169884169884 | global_auc: 0.9996763230296164| flobal_FPR: 0.011583011583011582 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 17 | global_acc: 99.079% | global_loss: 0.5725113749504089 | global_f1: 0.9865125240847784 | global_precision: 0.9846153846153847 | global_recall: 0.9884169884169884 | global_auc: 0.9996917362186822| flobal_FPR: 0.011583011583011582 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 18 | global_acc: 99.079% | global_loss: 0.5722625851631165 | global_f1: 0.9865125240847784 | global_precision: 0.9846153846153847 | global_recall: 0.9884169884169884 | global_auc: 0.999722562596814| flobal_FPR: 0.011583011583011582 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 19 | global_acc: 99.079% | global_loss: 0.5721981525421143 | global_f1: 0.9865125240847784 | global_precision: 0.9846153846153847 | global_recall: 0.9884169884169884 | global_auc: 0.999722562596814| flobal_FPR: 0.011583011583011582 \n",
            "---------------------------------------------\n",
            "No. of Clients: 10\n",
            "No. of Rounds: 20\n",
            "---------------------------------------------\n",
            "|=======================|\n",
            "|   DWFedAvg  2023      |\n",
            "|=======================|\n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 0 | global_acc: 93.816% | global_loss: 0.6688956022262573 | global_f1: 0.9014675052410901 | global_precision: 0.9862385321100917 | global_recall: 0.8301158301158301 | global_auc: 0.990312810672092| flobal_FPR: 0.16988416988416988 \n",
            "24/24 [==============================] - 0s 3ms/step\n",
            "comm_round: 1 | global_acc: 96.842% | global_loss: 0.6166717410087585 | global_f1: 0.9518072289156626 | global_precision: 0.9916317991631799 | global_recall: 0.915057915057915 | global_auc: 0.9958384389522115| flobal_FPR: 0.08494208494208494 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 2 | global_acc: 97.632% | global_loss: 0.601692259311676 | global_f1: 0.9647058823529412 | global_precision: 0.9800796812749004 | global_recall: 0.9498069498069498 | global_auc: 0.9969019489977573| flobal_FPR: 0.05019305019305019 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 3 | global_acc: 97.632% | global_loss: 0.5946711301803589 | global_f1: 0.9647058823529412 | global_precision: 0.9800796812749004 | global_recall: 0.9498069498069498 | global_auc: 0.997487650182261| flobal_FPR: 0.05019305019305019 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 4 | global_acc: 97.895% | global_loss: 0.5909357070922852 | global_f1: 0.96875 | global_precision: 0.9802371541501976 | global_recall: 0.9575289575289575 | global_auc: 0.9977727941799799| flobal_FPR: 0.04247104247104247 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 5 | global_acc: 98.026% | global_loss: 0.58854740858078 | global_f1: 0.9707602339181286 | global_precision: 0.9803149606299213 | global_recall: 0.9613899613899614 | global_auc: 0.9980810579612974| flobal_FPR: 0.03861003861003861 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 6 | global_acc: 98.289% | global_loss: 0.5869296193122864 | global_f1: 0.9746588693957116 | global_precision: 0.984251968503937 | global_recall: 0.9652509652509652 | global_auc: 0.9983430821754175| flobal_FPR: 0.03474903474903475 \n",
            "24/24 [==============================] - 0s 3ms/step\n",
            "comm_round: 7 | global_acc: 98.421% | global_loss: 0.5856896638870239 | global_f1: 0.9766536964980543 | global_precision: 0.984313725490196 | global_recall: 0.9691119691119691 | global_auc: 0.998404734931681| flobal_FPR: 0.03088803088803089 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 8 | global_acc: 98.421% | global_loss: 0.5844286680221558 | global_f1: 0.9766536964980543 | global_precision: 0.984313725490196 | global_recall: 0.9691119691119691 | global_auc: 0.9985434536332739| flobal_FPR: 0.03088803088803089 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 9 | global_acc: 98.684% | global_loss: 0.5837107300758362 | global_f1: 0.9806201550387598 | global_precision: 0.9844357976653697 | global_recall: 0.9768339768339769 | global_auc: 0.9986205195786034| flobal_FPR: 0.023166023166023165 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 10 | global_acc: 98.684% | global_loss: 0.5830254554748535 | global_f1: 0.9806201550387598 | global_precision: 0.9844357976653697 | global_recall: 0.9768339768339769 | global_auc: 0.9987129987129987| flobal_FPR: 0.023166023166023165 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 11 | global_acc: 98.816% | global_loss: 0.582414984703064 | global_f1: 0.9825918762088974 | global_precision: 0.9844961240310077 | global_recall: 0.9806949806949807 | global_auc: 0.9987669448747294| flobal_FPR: 0.019305019305019305 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 12 | global_acc: 98.816% | global_loss: 0.581752359867096 | global_f1: 0.9825918762088974 | global_precision: 0.9844961240310077 | global_recall: 0.9806949806949807 | global_auc: 0.9987977712528611| flobal_FPR: 0.019305019305019305 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 13 | global_acc: 98.816% | global_loss: 0.5812164545059204 | global_f1: 0.9825918762088974 | global_precision: 0.9844961240310077 | global_recall: 0.9806949806949807 | global_auc: 0.9988363042255258| flobal_FPR: 0.019305019305019305 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 14 | global_acc: 98.816% | global_loss: 0.5808151364326477 | global_f1: 0.9825918762088974 | global_precision: 0.9844961240310077 | global_recall: 0.9806949806949807 | global_auc: 0.9988594240091246| flobal_FPR: 0.019305019305019305 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 15 | global_acc: 98.816% | global_loss: 0.5805327296257019 | global_f1: 0.9825918762088974 | global_precision: 0.9844961240310077 | global_recall: 0.9806949806949807 | global_auc: 0.9989441965489869| flobal_FPR: 0.019305019305019305 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 16 | global_acc: 98.816% | global_loss: 0.5803776979446411 | global_f1: 0.9825918762088974 | global_precision: 0.9844961240310077 | global_recall: 0.9806949806949807 | global_auc: 0.9989673163325857| flobal_FPR: 0.019305019305019305 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 17 | global_acc: 98.816% | global_loss: 0.5800328254699707 | global_f1: 0.9825918762088974 | global_precision: 0.9844961240310077 | global_recall: 0.9806949806949807 | global_auc: 0.9989827295216517| flobal_FPR: 0.019305019305019305 \n",
            "24/24 [==============================] - 0s 3ms/step\n",
            "comm_round: 18 | global_acc: 98.816% | global_loss: 0.5798544883728027 | global_f1: 0.9825918762088974 | global_precision: 0.9844961240310077 | global_recall: 0.9806949806949807 | global_auc: 0.9989904361161847| flobal_FPR: 0.019305019305019305 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 19 | global_acc: 98.816% | global_loss: 0.5796003937721252 | global_f1: 0.9825918762088974 | global_precision: 0.9844961240310077 | global_recall: 0.9806949806949807 | global_auc: 0.9989981427107176| flobal_FPR: 0.019305019305019305 \n",
            "---------------------------------------------\n",
            "No. of Clients: 15\n",
            "No. of Rounds: 20\n",
            "---------------------------------------------\n",
            "|=======================|\n",
            "|   DWFedAvg  2023      |\n",
            "|=======================|\n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 0 | global_acc: 94.605% | global_loss: 0.6673741340637207 | global_f1: 0.9151138716356108 | global_precision: 0.9866071428571429 | global_recall: 0.8532818532818532 | global_auc: 0.9895190314351991| flobal_FPR: 0.14671814671814673 \n",
            "24/24 [==============================] - 0s 3ms/step\n",
            "comm_round: 1 | global_acc: 96.053% | global_loss: 0.6233183145523071 | global_f1: 0.9390243902439024 | global_precision: 0.9914163090128756 | global_recall: 0.8918918918918919 | global_auc: 0.9939888562643054| flobal_FPR: 0.10810810810810811 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 2 | global_acc: 96.842% | global_loss: 0.6093485951423645 | global_f1: 0.9518072289156626 | global_precision: 0.9916317991631799 | global_recall: 0.915057915057915 | global_auc: 0.9949984201481207| flobal_FPR: 0.08494208494208494 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 3 | global_acc: 97.105% | global_loss: 0.601966381072998 | global_f1: 0.9558232931726909 | global_precision: 0.99581589958159 | global_recall: 0.918918918918919 | global_auc: 0.9957228400342173| flobal_FPR: 0.08108108108108109 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 4 | global_acc: 97.105% | global_loss: 0.5979902744293213 | global_f1: 0.9560000000000001 | global_precision: 0.991701244813278 | global_recall: 0.9227799227799228 | global_auc: 0.996293128029655| flobal_FPR: 0.07722007722007722 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 5 | global_acc: 97.368% | global_loss: 0.5953068137168884 | global_f1: 0.9601593625498009 | global_precision: 0.9917695473251029 | global_recall: 0.9305019305019305 | global_auc: 0.9968788292141586| flobal_FPR: 0.0694980694980695 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 6 | global_acc: 97.368% | global_loss: 0.5930525660514832 | global_f1: 0.9601593625498009 | global_precision: 0.9917695473251029 | global_recall: 0.9305019305019305 | global_auc: 0.9970483742938833| flobal_FPR: 0.0694980694980695 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 7 | global_acc: 97.632% | global_loss: 0.5916833877563477 | global_f1: 0.9642857142857143 | global_precision: 0.9918367346938776 | global_recall: 0.9382239382239382 | global_auc: 0.9972641589408057| flobal_FPR: 0.06177606177606178 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 8 | global_acc: 97.763% | global_loss: 0.5903563499450684 | global_f1: 0.9663366336633663 | global_precision: 0.991869918699187 | global_recall: 0.9420849420849421 | global_auc: 0.9974028776423988| flobal_FPR: 0.05791505791505792 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 9 | global_acc: 97.763% | global_loss: 0.5894788503646851 | global_f1: 0.9663366336633663 | global_precision: 0.991869918699187 | global_recall: 0.9420849420849421 | global_auc: 0.9975261831549256| flobal_FPR: 0.05791505791505792 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 10 | global_acc: 97.763% | global_loss: 0.5886626243591309 | global_f1: 0.9663366336633663 | global_precision: 0.991869918699187 | global_recall: 0.9420849420849421 | global_auc: 0.9977882073690456| flobal_FPR: 0.05791505791505792 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 11 | global_acc: 97.763% | global_loss: 0.5879042744636536 | global_f1: 0.9663366336633663 | global_precision: 0.991869918699187 | global_recall: 0.9420849420849421 | global_auc: 0.9979423392597045| flobal_FPR: 0.05791505791505792 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 12 | global_acc: 97.895% | global_loss: 0.5874959826469421 | global_f1: 0.9683794466403162 | global_precision: 0.9919028340080972 | global_recall: 0.9459459459459459 | global_auc: 0.9980039920159681| flobal_FPR: 0.05405405405405406 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 13 | global_acc: 97.895% | global_loss: 0.5869969725608826 | global_f1: 0.9683794466403162 | global_precision: 0.9919028340080972 | global_recall: 0.9459459459459459 | global_auc: 0.9980656447722317| flobal_FPR: 0.05405405405405406 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 14 | global_acc: 97.895% | global_loss: 0.5863587260246277 | global_f1: 0.9683794466403162 | global_precision: 0.9919028340080972 | global_recall: 0.9459459459459459 | global_auc: 0.998127297528495| flobal_FPR: 0.05405405405405406 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 15 | global_acc: 97.895% | global_loss: 0.5859206914901733 | global_f1: 0.9683794466403162 | global_precision: 0.9919028340080972 | global_recall: 0.9459459459459459 | global_auc: 0.998150417312094| flobal_FPR: 0.05405405405405406 \n",
            "24/24 [==============================] - 0s 3ms/step\n",
            "comm_round: 16 | global_acc: 97.895% | global_loss: 0.5854904055595398 | global_f1: 0.9683794466403162 | global_precision: 0.9919028340080972 | global_recall: 0.9459459459459459 | global_auc: 0.9982120700683574| flobal_FPR: 0.05405405405405406 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 17 | global_acc: 98.026% | global_loss: 0.5850285887718201 | global_f1: 0.9704142011834319 | global_precision: 0.9919354838709677 | global_recall: 0.9498069498069498 | global_auc: 0.9982583096355552| flobal_FPR: 0.05019305019305019 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 18 | global_acc: 98.026% | global_loss: 0.5847582221031189 | global_f1: 0.9704142011834319 | global_precision: 0.9919354838709677 | global_recall: 0.9498069498069498 | global_auc: 0.9983353755808846| flobal_FPR: 0.05019305019305019 \n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "comm_round: 19 | global_acc: 98.026% | global_loss: 0.5844762921333313 | global_f1: 0.9704142011834319 | global_precision: 0.9919354838709677 | global_recall: 0.9498069498069498 | global_auc: 0.9983662019590164| flobal_FPR: 0.05019305019305019 \n",
            "===================================================================================================\n",
            "Working with: Kronodroid\n",
            "===================================================================================================\n",
            "---------------------------------------------\n",
            "No. of Clients: 5\n",
            "No. of Rounds: 10\n",
            "---------------------------------------------\n",
            "|=======================|\n",
            "|   DWFedAvg  2023      |\n",
            "|=======================|\n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 0 | global_acc: 91.701% | global_loss: 0.6103391647338867 | global_f1: 0.9211214498570821 | global_precision: 0.927722650986157 | global_recall: 0.9146135265700484 | global_auc: 0.966251152175228| flobal_FPR: 0.08538647342995169 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 1 | global_acc: 92.034% | global_loss: 0.5681371092796326 | global_f1: 0.92316237733753 | global_precision: 0.9439606209769027 | global_recall: 0.9032608695652173 | global_auc: 0.969750658432805| flobal_FPR: 0.0967391304347826 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 2 | global_acc: 92.513% | global_loss: 0.5582050681114197 | global_f1: 0.9282912478548664 | global_precision: 0.9423842707814833 | global_recall: 0.9146135265700484 | global_auc: 0.9729403758359435| flobal_FPR: 0.08538647342995169 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 3 | global_acc: 92.789% | global_loss: 0.553692102432251 | global_f1: 0.931116679909541 | global_precision: 0.9425813636926123 | global_recall: 0.9199275362318841 | global_auc: 0.9746844091790069| flobal_FPR: 0.08007246376811594 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 4 | global_acc: 92.955% | global_loss: 0.5506942868232727 | global_f1: 0.9326646688275947 | global_precision: 0.9447404286953289 | global_recall: 0.9208937198067633 | global_auc: 0.9759642769796375| flobal_FPR: 0.07910628019323672 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 5 | global_acc: 93.070% | global_loss: 0.5489042401313782 | global_f1: 0.9338383529842996 | global_precision: 0.9448633947335888 | global_recall: 0.9230676328502415 | global_auc: 0.9767120156919363| flobal_FPR: 0.07693236714975846 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 6 | global_acc: 93.083% | global_loss: 0.5472837090492249 | global_f1: 0.9339605351579204 | global_precision: 0.944987019409074 | global_recall: 0.9231884057971015 | global_auc: 0.9772886867897933| flobal_FPR: 0.07681159420289856 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 7 | global_acc: 93.121% | global_loss: 0.5461657643318176 | global_f1: 0.9343511450381679 | global_precision: 0.9450277949351451 | global_recall: 0.9239130434782609 | global_auc: 0.9777524249919134| flobal_FPR: 0.07608695652173914 \n",
            "489/489 [==============================] - 1s 3ms/step\n",
            "comm_round: 8 | global_acc: 93.185% | global_loss: 0.5453394055366516 | global_f1: 0.9349856541114706 | global_precision: 0.9453153931613381 | global_recall: 0.9248792270531401 | global_auc: 0.9781274736429645| flobal_FPR: 0.0751207729468599 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 9 | global_acc: 93.275% | global_loss: 0.5446550250053406 | global_f1: 0.9358559658223986 | global_precision: 0.9459592843923504 | global_recall: 0.9259661835748793 | global_auc: 0.9784375665664717| flobal_FPR: 0.07403381642512077 \n",
            "---------------------------------------------\n",
            "No. of Clients: 10\n",
            "No. of Rounds: 10\n",
            "---------------------------------------------\n",
            "|=======================|\n",
            "|   DWFedAvg  2023      |\n",
            "|=======================|\n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 0 | global_acc: 89.167% | global_loss: 0.615933358669281 | global_f1: 0.8954615622105588 | global_precision: 0.9161086544535691 | global_recall: 0.8757246376811594 | global_auc: 0.9540540624942473| flobal_FPR: 0.12427536231884058 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 1 | global_acc: 90.280% | global_loss: 0.5868694186210632 | global_f1: 0.9058451620901259 | global_precision: 0.9304724309181205 | global_recall: 0.8824879227053141 | global_auc: 0.9596470925080011| flobal_FPR: 0.11751207729468599 \n",
            "489/489 [==============================] - 1s 3ms/step\n",
            "comm_round: 2 | global_acc: 90.818% | global_loss: 0.5762213468551636 | global_f1: 0.9110849495012082 | global_precision: 0.9354879755694109 | global_recall: 0.8879227053140096 | global_auc: 0.9630065872865601| flobal_FPR: 0.11207729468599034 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 3 | global_acc: 91.074% | global_loss: 0.5706589221954346 | global_f1: 0.9138410227904391 | global_precision: 0.9351535836177475 | global_recall: 0.8934782608695652 | global_auc: 0.964979173734218| flobal_FPR: 0.10652173913043478 \n",
            "489/489 [==============================] - 1s 3ms/step\n",
            "comm_round: 4 | global_acc: 91.400% | global_loss: 0.5671167373657227 | global_f1: 0.9171903881700554 | global_precision: 0.9362264150943396 | global_recall: 0.8989130434782608 | global_auc: 0.9663158777964492| flobal_FPR: 0.10108695652173913 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 5 | global_acc: 91.611% | global_loss: 0.5646488666534424 | global_f1: 0.9193280413513014 | global_precision: 0.937147158449379 | global_recall: 0.9021739130434783 | global_auc: 0.9672644664557577| flobal_FPR: 0.09782608695652174 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 6 | global_acc: 91.803% | global_loss: 0.562738835811615 | global_f1: 0.9213483146067416 | global_precision: 0.9370550768077932 | global_recall: 0.9061594202898551 | global_auc: 0.9680048664199927| flobal_FPR: 0.09384057971014492 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 7 | global_acc: 91.906% | global_loss: 0.5611873269081116 | global_f1: 0.9223592954029337 | global_precision: 0.9377261949332335 | global_recall: 0.907487922705314 | global_auc: 0.9686265643942682| flobal_FPR: 0.092512077294686 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 8 | global_acc: 91.989% | global_loss: 0.560002326965332 | global_f1: 0.9231713303878253 | global_precision: 0.9383732534930139 | global_recall: 0.9084541062801932 | global_auc: 0.9691316313354845| flobal_FPR: 0.09154589371980676 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 9 | global_acc: 92.142% | global_loss: 0.5589572787284851 | global_f1: 0.9247087676272225 | global_precision: 0.9391033623910336 | global_recall: 0.9107487922705314 | global_auc: 0.9695805769883159| flobal_FPR: 0.0892512077294686 \n",
            "---------------------------------------------\n",
            "No. of Clients: 15\n",
            "No. of Rounds: 10\n",
            "---------------------------------------------\n",
            "|=======================|\n",
            "|   DWFedAvg  2023      |\n",
            "|=======================|\n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 0 | global_acc: 86.806% | global_loss: 0.6291088461875916 | global_f1: 0.8696751358867401 | global_precision: 0.9122248740387165 | global_recall: 0.8309178743961353 | global_auc: 0.937973731390973| flobal_FPR: 0.16908212560386474 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 1 | global_acc: 87.548% | global_loss: 0.6072027087211609 | global_f1: 0.876192899860033 | global_precision: 0.9257865017477817 | global_recall: 0.8316425120772947 | global_auc: 0.9473444497697622| flobal_FPR: 0.1683574879227053 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 2 | global_acc: 89.109% | global_loss: 0.5974348783493042 | global_f1: 0.8930367018602313 | global_precision: 0.9309486373165619 | global_recall: 0.8580917874396136 | global_auc: 0.9513558521954772| flobal_FPR: 0.14190821256038647 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 3 | global_acc: 89.564% | global_loss: 0.591495931148529 | global_f1: 0.8977621763931549 | global_precision: 0.9332725140101655 | global_recall: 0.8648550724637681 | global_auc: 0.9539898874845829| flobal_FPR: 0.1351449275362319 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 4 | global_acc: 89.960% | global_loss: 0.5874244570732117 | global_f1: 0.9017471350742062 | global_precision: 0.9364026531408506 | global_recall: 0.8695652173913043 | global_auc: 0.9558166111781707| flobal_FPR: 0.13043478260869565 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 5 | global_acc: 90.210% | global_loss: 0.5844215154647827 | global_f1: 0.904351087771943 | global_precision: 0.937402799377916 | global_recall: 0.8735507246376811 | global_auc: 0.9570682498639086| flobal_FPR: 0.12644927536231884 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 6 | global_acc: 90.344% | global_loss: 0.5819745063781738 | global_f1: 0.9057522952969832 | global_precision: 0.9379123011253395 | global_recall: 0.8757246376811594 | global_auc: 0.9581483114140625| flobal_FPR: 0.12427536231884058 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 7 | global_acc: 90.447% | global_loss: 0.5800261497497559 | global_f1: 0.9067981771646171 | global_precision: 0.9384933453934617 | global_recall: 0.8771739130434782 | global_auc: 0.9589993103384797| flobal_FPR: 0.12282608695652174 \n",
            "489/489 [==============================] - 1s 3ms/step\n",
            "comm_round: 8 | global_acc: 90.639% | global_loss: 0.5783983469009399 | global_f1: 0.9088416723783412 | global_precision: 0.9387308533916849 | global_recall: 0.8807971014492754 | global_auc: 0.9596924316715711| flobal_FPR: 0.11920289855072463 \n",
            "489/489 [==============================] - 1s 3ms/step\n",
            "comm_round: 9 | global_acc: 90.703% | global_loss: 0.5769796967506409 | global_f1: 0.9094760451062238 | global_precision: 0.9392613563247973 | global_recall: 0.8815217391304347 | global_auc: 0.9603210492716807| flobal_FPR: 0.11847826086956521 \n",
            "---------------------------------------------\n",
            "No. of Clients: 5\n",
            "No. of Rounds: 20\n",
            "---------------------------------------------\n",
            "|=======================|\n",
            "|   DWFedAvg  2023      |\n",
            "|=======================|\n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 0 | global_acc: 92.846% | global_loss: 0.5849939584732056 | global_f1: 0.9322424242424242 | global_precision: 0.9356447688564477 | global_recall: 0.9288647342995169 | global_auc: 0.9744034247052666| flobal_FPR: 0.07113526570048309 \n",
            "489/489 [==============================] - 1s 3ms/step\n",
            "comm_round: 1 | global_acc: 93.249% | global_loss: 0.545771062374115 | global_f1: 0.9355095054709947 | global_precision: 0.9471469241242728 | global_recall: 0.9241545893719807 | global_auc: 0.9774247782432499| flobal_FPR: 0.07584541062801932 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 2 | global_acc: 93.492% | global_loss: 0.5407693386077881 | global_f1: 0.9379764591083736 | global_precision: 0.9473943575212517 | global_recall: 0.928743961352657 | global_auc: 0.9794723957881338| flobal_FPR: 0.07125603864734299 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 3 | global_acc: 93.582% | global_loss: 0.5382032990455627 | global_f1: 0.9388078823744739 | global_precision: 0.9485883368265319 | global_recall: 0.9292270531400966 | global_auc: 0.9803572943046714| flobal_FPR: 0.07077294685990339 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 4 | global_acc: 93.838% | global_loss: 0.5367141962051392 | global_f1: 0.9412339049246354 | global_precision: 0.9512766744788455 | global_recall: 0.9314009661835749 | global_auc: 0.9809588744119141| flobal_FPR: 0.06859903381642513 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 5 | global_acc: 93.902% | global_loss: 0.5360930562019348 | global_f1: 0.9419079548918012 | global_precision: 0.9508923076923077 | global_recall: 0.9330917874396135 | global_auc: 0.9813614454227251| flobal_FPR: 0.06690821256038647 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 6 | global_acc: 93.998% | global_loss: 0.5352163910865784 | global_f1: 0.942797902183193 | global_precision: 0.9522049765952205 | global_recall: 0.9335748792270532 | global_auc: 0.9816990689240754| flobal_FPR: 0.06642512077294686 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 7 | global_acc: 94.113% | global_loss: 0.5347036123275757 | global_f1: 0.9439297903461725 | global_precision: 0.952755905511811 | global_recall: 0.9352657004830918 | global_auc: 0.9819457675557975| flobal_FPR: 0.0647342995169082 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 8 | global_acc: 94.177% | global_loss: 0.5343106389045715 | global_f1: 0.9445595223589619 | global_precision: 0.9530366363412835 | global_recall: 0.936231884057971 | global_auc: 0.982142927583568| flobal_FPR: 0.06376811594202898 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 9 | global_acc: 94.171% | global_loss: 0.533938467502594 | global_f1: 0.9444816868791516 | global_precision: 0.9532537827531061 | global_recall: 0.9358695652173913 | global_auc: 0.9823530804004639| flobal_FPR: 0.0641304347826087 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 10 | global_acc: 94.209% | global_loss: 0.5336573123931885 | global_f1: 0.9448338921060653 | global_precision: 0.9538461538461539 | global_recall: 0.9359903381642513 | global_auc: 0.9825005374626242| flobal_FPR: 0.0640096618357488 \n",
            "489/489 [==============================] - 1s 3ms/step\n",
            "comm_round: 11 | global_acc: 94.216% | global_loss: 0.5334800481796265 | global_f1: 0.9448982079726929 | global_precision: 0.9538518336204774 | global_recall: 0.9361111111111111 | global_auc: 0.9826254769775338| flobal_FPR: 0.06388888888888888 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 12 | global_acc: 94.254% | global_loss: 0.5333394408226013 | global_f1: 0.9452772699573431 | global_precision: 0.9539975399753997 | global_recall: 0.9367149758454106 | global_auc: 0.9827313423219438| flobal_FPR: 0.06328502415458938 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 13 | global_acc: 94.286% | global_loss: 0.53310626745224 | global_f1: 0.9455587392550143 | global_precision: 0.9546965406869383 | global_recall: 0.9365942028985508 | global_auc: 0.9828256037332449| flobal_FPR: 0.06340579710144928 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 14 | global_acc: 94.305% | global_loss: 0.5328257083892822 | global_f1: 0.9457250884254177 | global_precision: 0.9551613697955161 | global_recall: 0.9364734299516908 | global_auc: 0.9829262588130722| flobal_FPR: 0.06352657004830918 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 15 | global_acc: 94.331% | global_loss: 0.5326249599456787 | global_f1: 0.9459624298609418 | global_precision: 0.9555199605717102 | global_recall: 0.9365942028985508 | global_auc: 0.9830277110469444| flobal_FPR: 0.06340579710144928 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 16 | global_acc: 94.375% | global_loss: 0.5324699878692627 | global_f1: 0.9463795522479107 | global_precision: 0.9561198077160113 | global_recall: 0.9368357487922705 | global_auc: 0.9830992165865896| flobal_FPR: 0.06316425120772946 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 17 | global_acc: 94.388% | global_loss: 0.5323426723480225 | global_f1: 0.946495027759136 | global_precision: 0.9563555665146097 | global_recall: 0.9368357487922705 | global_auc: 0.9831637778461522| flobal_FPR: 0.06316425120772946 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 18 | global_acc: 94.401% | global_loss: 0.5321543216705322 | global_f1: 0.9465974977113213 | global_precision: 0.9568167797655768 | global_recall: 0.9365942028985508 | global_auc: 0.9832239588675088| flobal_FPR: 0.06340579710144928 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 19 | global_acc: 94.407% | global_loss: 0.5320560932159424 | global_f1: 0.9466552734375 | global_precision: 0.9569348469891412 | global_recall: 0.9365942028985508 | global_auc: 0.9832794720177562| flobal_FPR: 0.06340579710144928 \n",
            "---------------------------------------------\n",
            "No. of Clients: 10\n",
            "No. of Rounds: 20\n",
            "---------------------------------------------\n",
            "|=======================|\n",
            "|   DWFedAvg  2023      |\n",
            "|=======================|\n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 0 | global_acc: 91.835% | global_loss: 0.588338315486908 | global_f1: 0.9218328840970351 | global_precision: 0.9353555445052213 | global_recall: 0.908695652173913 | global_auc: 0.963737273476762| flobal_FPR: 0.09130434782608696 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 1 | global_acc: 92.533% | global_loss: 0.5606169700622559 | global_f1: 0.9284005153690411 | global_precision: 0.943509165731388 | global_recall: 0.913768115942029 | global_auc: 0.9692546724732354| flobal_FPR: 0.08623188405797101 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 2 | global_acc: 92.808% | global_loss: 0.5535147190093994 | global_f1: 0.9311105663152734 | global_precision: 0.9452463912394226 | global_recall: 0.9173913043478261 | global_auc: 0.9719634923170787| flobal_FPR: 0.08260869565217391 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 3 | global_acc: 92.949% | global_loss: 0.5501862168312073 | global_f1: 0.9325416258570031 | global_precision: 0.945506454816286 | global_recall: 0.9199275362318841 | global_auc: 0.9734183888481271| flobal_FPR: 0.08007246376811594 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 4 | global_acc: 93.147% | global_loss: 0.5480238795280457 | global_f1: 0.9344513128098415 | global_precision: 0.9472639285271125 | global_recall: 0.9219806763285024 | global_auc: 0.974419606110572| flobal_FPR: 0.07801932367149758 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 5 | global_acc: 93.243% | global_loss: 0.5464728474617004 | global_f1: 0.9353495775682624 | global_precision: 0.9484728085423392 | global_recall: 0.922584541062802 | global_auc: 0.9751599403301434| flobal_FPR: 0.07741545893719806 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 6 | global_acc: 93.326% | global_loss: 0.5453137159347534 | global_f1: 0.9361962439591364 | global_precision: 0.9485558447998017 | global_recall: 0.9241545893719807 | global_auc: 0.9757245719364959| flobal_FPR: 0.07584541062801932 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 7 | global_acc: 93.358% | global_loss: 0.5443856120109558 | global_f1: 0.9365137614678898 | global_precision: 0.9486988847583643 | global_recall: 0.9246376811594202 | global_auc: 0.9761434640600222| flobal_FPR: 0.07536231884057971 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 8 | global_acc: 93.396% | global_loss: 0.5437125563621521 | global_f1: 0.9368807339449542 | global_precision: 0.9490706319702602 | global_recall: 0.925 | global_auc: 0.9765165239350023| flobal_FPR: 0.075 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 9 | global_acc: 93.422% | global_loss: 0.5430886149406433 | global_f1: 0.937125382262997 | global_precision: 0.9493184634448575 | global_recall: 0.9252415458937198 | global_auc: 0.9767904572935816| flobal_FPR: 0.0747584541062802 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 10 | global_acc: 93.441% | global_loss: 0.5425885319709778 | global_f1: 0.9372896910370144 | global_precision: 0.9497830130192189 | global_recall: 0.9251207729468599 | global_auc: 0.9770358656862823| flobal_FPR: 0.0748792270531401 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 11 | global_acc: 93.460% | global_loss: 0.5421102046966553 | global_f1: 0.9374617549871497 | global_precision: 0.9501364425700819 | global_recall: 0.9251207729468599 | global_auc: 0.977274083256412| flobal_FPR: 0.0748792270531401 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 12 | global_acc: 93.492% | global_loss: 0.5416842699050903 | global_f1: 0.9377867498623602 | global_precision: 0.9501673484566754 | global_recall: 0.9257246376811594 | global_auc: 0.9774840224031515| flobal_FPR: 0.07427536231884058 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 13 | global_acc: 93.499% | global_loss: 0.541441023349762 | global_f1: 0.937866927592955 | global_precision: 0.9499504459861249 | global_recall: 0.9260869565217391 | global_auc: 0.9776762104907446| flobal_FPR: 0.07391304347826087 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 14 | global_acc: 93.524% | global_loss: 0.5411786437034607 | global_f1: 0.938126681340181 | global_precision: 0.9499752352649826 | global_recall: 0.9265700483091788 | global_auc: 0.9778319185081746| flobal_FPR: 0.07342995169082125 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 15 | global_acc: 93.531% | global_loss: 0.5409055352210999 | global_f1: 0.9381764813795633 | global_precision: 0.9502043849869937 | global_recall: 0.9264492753623188 | global_auc: 0.9779786934694511| flobal_FPR: 0.07355072463768116 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 16 | global_acc: 93.537% | global_loss: 0.540649950504303 | global_f1: 0.9382414088296442 | global_precision: 0.9502105523903889 | global_recall: 0.9265700483091788 | global_auc: 0.978107298249351| flobal_FPR: 0.07342995169082125 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 17 | global_acc: 93.563% | global_loss: 0.5403791069984436 | global_f1: 0.9384709480122325 | global_precision: 0.9506815365551425 | global_recall: 0.9265700483091788 | global_auc: 0.9782358783750023| flobal_FPR: 0.07342995169082125 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 18 | global_acc: 93.576% | global_loss: 0.5401683449745178 | global_f1: 0.9385932721712539 | global_precision: 0.9508054522924412 | global_recall: 0.9266908212560386 | global_auc: 0.9783770650398808| flobal_FPR: 0.07330917874396135 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 19 | global_acc: 93.608% | global_loss: 0.5400388240814209 | global_f1: 0.9389252307880419 | global_precision: 0.9507242788163922 | global_recall: 0.9274154589371981 | global_auc: 0.9784859792930607| flobal_FPR: 0.07258454106280193 \n",
            "---------------------------------------------\n",
            "No. of Clients: 15\n",
            "No. of Rounds: 20\n",
            "---------------------------------------------\n",
            "|=======================|\n",
            "|   DWFedAvg  2023      |\n",
            "|=======================|\n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 0 | global_acc: 89.813% | global_loss: 0.593777060508728 | global_f1: 0.9018495684340321 | global_precision: 0.9211586901763225 | global_recall: 0.8833333333333333 | global_auc: 0.9541605524129607| flobal_FPR: 0.11666666666666667 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 1 | global_acc: 91.189% | global_loss: 0.5711994767189026 | global_f1: 0.9152041381858488 | global_precision: 0.9336600075386355 | global_recall: 0.8974637681159421 | global_auc: 0.9619381214514318| flobal_FPR: 0.10253623188405797 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 2 | global_acc: 91.701% | global_loss: 0.5635988116264343 | global_f1: 0.9203170117343492 | global_precision: 0.9366012254595474 | global_recall: 0.9045893719806763 | global_auc: 0.9655269089620495| flobal_FPR: 0.09541062801932366 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 3 | global_acc: 92.040% | global_loss: 0.559542179107666 | global_f1: 0.9236903447429763 | global_precision: 0.9385440039890302 | global_recall: 0.9092995169082125 | global_auc: 0.9675663988229075| flobal_FPR: 0.09070048309178744 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 4 | global_acc: 92.385% | global_loss: 0.5568834543228149 | global_f1: 0.9271547502448578 | global_precision: 0.9400446871896723 | global_recall: 0.9146135265700484 | global_auc: 0.969042604843015| flobal_FPR: 0.08538647342995169 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 5 | global_acc: 92.571% | global_loss: 0.5549811124801636 | global_f1: 0.9290645811694263 | global_precision: 0.9401508594039817 | global_recall: 0.9182367149758454 | global_auc: 0.9700370586232016| flobal_FPR: 0.08176328502415459 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 6 | global_acc: 92.673% | global_loss: 0.5536801815032959 | global_f1: 0.9301104803759995 | global_precision: 0.9402690361594471 | global_recall: 0.9201690821256039 | global_auc: 0.9707611046023895| flobal_FPR: 0.07983091787439614 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 7 | global_acc: 92.776% | global_loss: 0.5525344014167786 | global_f1: 0.9310534351145038 | global_precision: 0.941692402717727 | global_recall: 0.9206521739130434 | global_auc: 0.971414302488567| flobal_FPR: 0.07934782608695652 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 8 | global_acc: 92.846% | global_loss: 0.5515305399894714 | global_f1: 0.931712680185683 | global_precision: 0.9425358378645576 | global_recall: 0.9211352657004831 | global_auc: 0.972013318553933| flobal_FPR: 0.0788647342995169 \n",
            "489/489 [==============================] - 1s 2ms/step\n",
            "comm_round: 9 | global_acc: 92.917% | global_loss: 0.5508327484130859 | global_f1: 0.9323886886948024 | global_precision: 0.9431607562090696 | global_recall: 0.9218599033816425 | global_auc: 0.9724655596580226| flobal_FPR: 0.07814009661835748 \n"
          ]
        }
      ],
      "source": [
        "n_clients = [5,10,15]\n",
        "n_round = [10,20]\n",
        "\n",
        "all_avg =[]\n",
        "\n",
        "all_std =[]\n",
        "\n",
        "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd' ]\n",
        "\n",
        "\n",
        "for d in range(0,len(dataset)):\n",
        "    if d == 0:\n",
        "        use_data = Drebin_data\n",
        "    elif d==1:\n",
        "        use_data = Malgenome_data\n",
        "    elif d==2:\n",
        "        use_data = Kronodroid_data\n",
        "    elif d==3:\n",
        "        use_data = Tuandromd_data\n",
        "        \n",
        "        \n",
        "    print('===================================================================================================')\n",
        "    print('Working with:',dataset[d])\n",
        "    print('===================================================================================================')\n",
        "\n",
        "    for r in n_round: #number of rounds loop\n",
        "        comms_round = r\n",
        "        for cl in n_clients: #number of clients loop\n",
        "            number_of_clients = cl\n",
        "\n",
        "            # from sklearn.utils import shuffle\n",
        "            # use_data = shuffle(use_data)\n",
        "            # use_data\n",
        "            print('---------------------------------------------')\n",
        "            print('No. of Clients:', number_of_clients)\n",
        "            print('No. of Rounds:', comms_round)\n",
        "            print('---------------------------------------------')\n",
        "\n",
        "\n",
        "            features = np.array(use_data.iloc[:,range(0,use_data.shape[1]-1)]) #feature set\n",
        "\n",
        "            labels = use_data.iloc[:,-1] #labels --> B : Benign and S\n",
        "\n",
        "\n",
        "            #Do feature scaling \n",
        "\n",
        "\n",
        "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
        "\n",
        "\n",
        "            #binarize the labels\n",
        "            lb = LabelBinarizer()\n",
        "            y = lb.fit_transform(labels)\n",
        "\n",
        "\n",
        "            #split data into training and test set\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, \n",
        "                                                                y, shuffle=True,\n",
        "                                                                test_size=0.2, \n",
        "                                                                random_state=100)\n",
        "\n",
        "\n",
        "\n",
        "            #create clients -- Horizontal FL\n",
        "            clients = create_clients(X_train, y_train, num_clients=number_of_clients, initial='client')\n",
        "\n",
        "            #process and batch the training data for each client\n",
        "            clients_batched = dict()\n",
        "            for (client_name, data) in clients.items():\n",
        "                clients_batched[client_name] = batch_data(data)\n",
        "\n",
        "\n",
        "                #process and batch the test set  \n",
        "            test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))\n",
        "\n",
        "\n",
        "\n",
        "            #==============================================\n",
        "            # DW-FedAvg 2023\n",
        "            #==============================================\n",
        "            #-----------------------------------------------\n",
        "\n",
        "\n",
        "            all_results=list()\n",
        "\n",
        "            #create optimizer\n",
        "            lr = 0.01 \n",
        "            loss='binary_crossentropy'\n",
        "            metrics = ['accuracy']\n",
        "            optimizer = SGD(learning_rate=lr, \n",
        "                            decay=lr / comms_round)\n",
        "            # from tensorflow.keras.optimizers import SGD\n",
        "            # from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "\n",
        "            # def lr_schedule(epoch, lr):\n",
        "            #     decay_rate = lr / comms_round\n",
        "            #     return lr * (1. / (1. + decay_rate * epoch))\n",
        "\n",
        "            # lr = 0.01 \n",
        "            # loss = 'binary_crossentropy'\n",
        "            # metrics = ['accuracy']\n",
        "            # optimizer = SGD(learning_rate=lr, momentum=0.9)\n",
        "            # callbacks = [LearningRateScheduler(lr_schedule)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            #initialize global model\n",
        "            smlp_global = SimpleMLP()\n",
        "            global_model = smlp_global.build(X.shape[1],1)\n",
        "\n",
        "            global_DWFed_Avg_index = np.ones((1,number_of_clients))/number_of_clients # [0.3 0.3 0.3]\n",
        "\n",
        "            prev_local_model_test_performance = []\n",
        "\n",
        "            current_local_model_test_performance = []\n",
        "\n",
        "            beta= 0.2\n",
        "            #-----------------------------------------------\n",
        "\n",
        "\n",
        "            print('|=======================|')\n",
        "            print('|   DWFedAvg  2023      |')\n",
        "            print('|=======================|')\n",
        "\n",
        "            #commence global training loop\n",
        "            for comm_round in range(comms_round):\n",
        "\n",
        "                # get the global model's weights - will serve as the initial weights for all local models\n",
        "                global_weights = global_model.get_weights()\n",
        "\n",
        "                #initial list to collect local model weights after scalling\n",
        "                scaled_local_weight_list = list()\n",
        "\n",
        "                #randomize client data - using keys\n",
        "                client_names= list(clients_batched.keys())\n",
        "                random.shuffle(client_names)\n",
        "\n",
        "                #loop through each client and create new local model\n",
        "                for client in client_names:\n",
        "                    smlp_local = SimpleMLP()\n",
        "                    local_model = smlp_local.build(X.shape[1],1)\n",
        "                    local_model.compile(loss=loss, \n",
        "                                  optimizer=optimizer, \n",
        "                                  metrics=metrics)\n",
        "\n",
        "                    #set local model weight to the weight of the global model\n",
        "                    local_model.set_weights(global_weights)\n",
        "\n",
        "                    #============================================================    \n",
        "                    # Custom Code added HERE for DWFedAvg\n",
        "                    #============================================================\n",
        "                    x_data = list()\n",
        "                    y_data = list()\n",
        "                    for local_i in clients_batched[client]:\n",
        "                        for local_j in local_i[0].numpy():\n",
        "                            x_data.append(local_j)\n",
        "                        for local_k in local_i[1].numpy():\n",
        "                            y_data.append(local_k[0])\n",
        "\n",
        "                    x_data = np.array(x_data) #x_TRAIN for corresponding client\n",
        "                    y_data = np.array(y_data) #y_TRAIN for corresponding client        \n",
        "\n",
        "                    #============================================================\n",
        "\n",
        "                    #fit local model with client's data\n",
        "                    history = local_model.fit(x_data,y_data, epochs=32, verbose=0, validation_split=0.2)\n",
        "\n",
        "                    current_local_model_test_performance.append(np.mean(history.history['val_accuracy'])) #saving local model test performmance\n",
        "\n",
        "\n",
        "\n",
        "                    #scale the model weights and add to list\n",
        "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
        "                    scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
        "                    scaled_local_weight_list.append(scaled_weights)\n",
        "\n",
        "                    #clear session to free memory after each communication round\n",
        "                    K.clear_session()\n",
        "\n",
        "\n",
        "\n",
        "                if comm_round ==0:\n",
        "                    prev_local_model_test_performance = current_local_model_test_performance\n",
        "\n",
        "                else:\n",
        "                    for jj in range(0,len(prev_local_model_test_performance)):\n",
        "\n",
        "                        #performance immproved\n",
        "                        if (current_local_model_test_performance[jj]>prev_local_model_test_performance[jj]):\n",
        "                            #print('YES')\n",
        "                            global_DWFed_Avg_index[jj] = global_DWFed_Avg_index[jj] + global_DWFed_Avg_index[jj]*beta\n",
        "\n",
        "                        #performance degraded\n",
        "                        elif (current_local_model_test_performance[jj]<prev_local_model_test_performance[jj]):\n",
        "                            #print('NO')\n",
        "                            global_DWFed_Avg_index[jj] = global_DWFed_Avg_index[jj] - global_DWFed_Avg_index[jj]*beta\n",
        "\n",
        "\n",
        "                            #print('NO CHANGE')\n",
        "\n",
        "                    global_DWFed_Avg_index = global_DWFed_Avg_index / sum(global_DWFed_Avg_index) #Weight scaling\n",
        "\n",
        "\n",
        "            #     print(global_DWFed_Avg_index)\n",
        "\n",
        "\n",
        "\n",
        "                #to get the average over all the local model, we simply take the sum of the scaled weights\n",
        "                average_weights = sum_scaled_weights(scaled_local_weight_list, global_DWFed_Avg_index)\n",
        "                #print(scaled_local_weight_list)\n",
        "\n",
        "                #update global model \n",
        "                global_model.set_weights(average_weights)\n",
        "\n",
        "                #test global model and print out metrics after each communications round\n",
        "                for(X_test, Y_test) in test_batched:\n",
        "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test, Y_test, global_model, comm_round)\n",
        "                    all_results.append([global_acc,global_loss.numpy(),global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
        "\n",
        "\n",
        "\n",
        "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
        "            flname = 'results/round-'+str(r)+'/'+str(cl)+'-clients/DW-FedAvg-'+dataset[d]+'-results.csv'\n",
        "            all_R.to_csv(flname, index=None)\n",
        "            \n",
        "            all_avg.append(np.concatenate(([dataset[d],r,cl],np.mean(all_results,axis=0)))) #Storing avg values for each dataset\n",
        "            all_std.append(np.concatenate(([dataset[d],r,cl],np.std(all_results,axis=0)))) #Storing std values sfor each dataset\n",
        "            \n",
        "\n",
        "            \n",
        "ALL_AVG = pd.DataFrame(all_avg, columns = ['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
        "ALL_AVG.to_csv('DW-FedAvg-all-avg-results.csv')\n",
        "\n",
        "ALL_STD = pd.DataFrame(all_std, columns = ['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
        "ALL_STD.to_csv('DW-FedAvg-all-std-results.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o__8PCwWtPLm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmkMA9KRGnFH"
      },
      "source": [
        "# Deep Learning Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by3Xy7Qj9xFq"
      },
      "source": [
        "## Simple Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Y5a7MMs9s9k"
      },
      "source": [
        "### Drebin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wkr8myDpGCU-",
        "outputId": "5a39949a-24ca-43b7-f64e-651297cbaa88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "301/301 [==============================] - 2s 4ms/step - loss: 1.0766 - accuracy: 0.9005 - val_loss: 0.5661 - val_accuracy: 0.9568\n",
            "Epoch 2/10\n",
            "301/301 [==============================] - 1s 4ms/step - loss: 0.7760 - accuracy: 0.9379 - val_loss: 0.5211 - val_accuracy: 0.9609\n",
            "Epoch 3/10\n",
            "301/301 [==============================] - 1s 4ms/step - loss: 0.6280 - accuracy: 0.9530 - val_loss: 0.4999 - val_accuracy: 0.9647\n",
            "Epoch 4/10\n",
            "301/301 [==============================] - 1s 4ms/step - loss: 0.7323 - accuracy: 0.9470 - val_loss: 0.5231 - val_accuracy: 0.9609\n",
            "Epoch 5/10\n",
            "301/301 [==============================] - 1s 4ms/step - loss: 0.7399 - accuracy: 0.9470 - val_loss: 0.5609 - val_accuracy: 0.9609\n",
            "Epoch 6/10\n",
            "301/301 [==============================] - 2s 6ms/step - loss: 0.9818 - accuracy: 0.9322 - val_loss: 1.1127 - val_accuracy: 0.8911\n",
            "Epoch 6: early stopping\n",
            "Test loss: 0.9855576157569885\n",
            "Test accuracy: 0.9059175252914429\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Load the Drebin dataset\n",
        "drebin_df = pd.read_csv('/content/drive/MyDrive/drebin.csv')\n",
        "\n",
        "# feature set\n",
        "features = np.array(Drebin_data.iloc[:, range(0, Drebin_data.shape[1]-1)])\n",
        "# labels --> B: Benign and S\n",
        "labels = Drebin_data.iloc[:, -1]\n",
        "\n",
        "# feature scaling\n",
        "X = preprocessing.StandardScaler().fit(features).transform(features)\n",
        "\n",
        "# binarize the labels\n",
        "lb = LabelBinarizer()\n",
        "y = lb.fit_transform(labels)\n",
        "\n",
        "# split data into training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.2, random_state=100)\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(X_train.shape[1],), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='relu'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[early_stop])\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f'Test loss: {score[0]}')\n",
        "print(f'Test accuracy: {score[1]}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# #declear path to your data\n",
        "# drebin_data_path = '/content/drive/MyDrive/drebin.csv'\n",
        "# malgenome_data_path = '/content/drive/MyDrive/malgenome.csv'\n",
        "# kronodroid_data_path = '/content/drive/MyDrive/kronodroid.csv'\n",
        "# TUANDROMD_data_path='/content/drive/MyDrive/TUANDROMD.csv'\n",
        "\n",
        "\n",
        "\n",
        "# Drebin_data = pd.read_csv(drebin_data_path, header = None)\n",
        "\n",
        "# Malgenome_data = pd.read_csv(malgenome_data_path)\n",
        "\n",
        "# Tuandromd_data=pd.read_csv(TUANDROMD_data_path)\n",
        "\n",
        "# kronodroid_data=pd.read_csv(kronodroid_data_path)\n",
        "# Kronodroid_data = kronodroid_data.iloc[:,range(1,kronodroid_data.shape[1])]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFln6hsl941o"
      },
      "source": [
        "### Malgenome"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jU68kwz_M0HS",
        "outputId": "f0be7276-7c6a-4303-8b6c-5fba02b3424a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "76/76 [==============================] - 2s 7ms/step - loss: 0.8644 - accuracy: 0.9144 - val_loss: 0.3624 - val_accuracy: 0.9737\n",
            "Epoch 2/10\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.3937 - accuracy: 0.9642 - val_loss: 0.3617 - val_accuracy: 0.9720\n",
            "Epoch 3/10\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.2757 - accuracy: 0.9778 - val_loss: 0.3853 - val_accuracy: 0.9737\n",
            "Epoch 4/10\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.3606 - accuracy: 0.9741 - val_loss: 0.3603 - val_accuracy: 0.9737\n",
            "Epoch 5/10\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.3231 - accuracy: 0.9757 - val_loss: 0.2583 - val_accuracy: 0.9819\n",
            "Epoch 6/10\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.3154 - accuracy: 0.9770 - val_loss: 0.3816 - val_accuracy: 0.9753\n",
            "Epoch 7/10\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.3095 - accuracy: 0.9766 - val_loss: 0.4065 - val_accuracy: 0.9737\n",
            "Epoch 8/10\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.5117 - accuracy: 0.9605 - val_loss: 0.5085 - val_accuracy: 0.9655\n",
            "Epoch 8: early stopping\n",
            "Test loss: 0.5726343393325806\n",
            "Test accuracy: 0.9605262875556946\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Load the Drebin dataset\n",
        "Malgenome_df = pd.read_csv('/content/drive/MyDrive/malgenome.csv')\n",
        "\n",
        "# feature set\n",
        "features = np.array(Malgenome_data.iloc[:, range(0, Drebin_data.shape[1]-1)])\n",
        "# labels --> B: Benign and S\n",
        "labels = Malgenome_data.iloc[:, -1]\n",
        "\n",
        "# feature scaling\n",
        "X = preprocessing.StandardScaler().fit(features).transform(features)\n",
        "\n",
        "# binarize the labels\n",
        "lb = LabelBinarizer()\n",
        "y = lb.fit_transform(labels)\n",
        "\n",
        "# split data into training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.2, random_state=100)\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(X_train.shape[1],), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='relu'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[early_stop])\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f'Test loss: {score[0]}')\n",
        "print(f'Test accuracy: {score[1]}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCuY5Yyl-cGK"
      },
      "source": [
        "### Tuandromd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocExs_xO-dgg",
        "outputId": "b491c135-b515-4ad4-e907-79b3a807c0ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "90/90 [==============================] - 2s 6ms/step - loss: 3.1941 - accuracy: 0.7833 - val_loss: 3.0925 - val_accuracy: 0.7972\n",
            "Epoch 2/10\n",
            "90/90 [==============================] - 0s 4ms/step - loss: 3.1449 - accuracy: 0.7938 - val_loss: 3.0925 - val_accuracy: 0.7972\n",
            "Epoch 3/10\n",
            "90/90 [==============================] - 0s 4ms/step - loss: 3.1449 - accuracy: 0.7938 - val_loss: 3.0925 - val_accuracy: 0.7972\n",
            "Epoch 4/10\n",
            "90/90 [==============================] - 0s 4ms/step - loss: 3.1449 - accuracy: 0.7938 - val_loss: 3.0925 - val_accuracy: 0.7972\n",
            "Epoch 4: early stopping\n",
            "Test loss: 2.81760835647583\n",
            "Test accuracy: 0.8152295351028442\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Load the Drebin dataset\n",
        "TUANDROMD_df = pd.read_csv('/content/drive/MyDrive/TUANDROMD.csv')\n",
        "\n",
        "# feature set\n",
        "features = np.array(Tuandromd_data.iloc[:, range(0, Drebin_data.shape[1]-1)])\n",
        "# labels --> B: Benign and S\n",
        "labels = Tuandromd_data.iloc[:, -1]\n",
        "\n",
        "# feature scaling\n",
        "X = preprocessing.StandardScaler().fit(features).transform(features)\n",
        "\n",
        "# binarize the labels\n",
        "lb = LabelBinarizer()\n",
        "y = lb.fit_transform(labels)\n",
        "\n",
        "# split data into training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.2, random_state=100)\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(X_train.shape[1],), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='relu'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[early_stop])\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f'Test loss: {score[0]}')\n",
        "print(f'Test accuracy: {score[1]}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbSJu0Cg-2LQ"
      },
      "source": [
        "### kronodroid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYQmTbHi-35p",
        "outputId": "74e13b8d-381f-407e-af21-ca9a625f1cdc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1563/1563 [==============================] - 10s 5ms/step - loss: 0.4981 - accuracy: 0.9457 - val_loss: 0.1429 - val_accuracy: 0.9839\n",
            "Epoch 2/10\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.2730 - accuracy: 0.9720 - val_loss: 0.0459 - val_accuracy: 0.9906\n",
            "Epoch 3/10\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.2755 - accuracy: 0.9754 - val_loss: 0.0697 - val_accuracy: 0.9921\n",
            "Epoch 4/10\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.3016 - accuracy: 0.9746 - val_loss: 0.1274 - val_accuracy: 0.9886\n",
            "Epoch 5/10\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.2012 - accuracy: 0.9825 - val_loss: 0.3203 - val_accuracy: 0.9731\n",
            "Epoch 5: early stopping\n",
            "Test loss: 0.3521949052810669\n",
            "Test accuracy: 0.9699897766113281\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Load the Drebin dataset\n",
        "kronodroid_df = pd.read_csv('/content/drive/MyDrive/kronodroid.csv')\n",
        "\n",
        "# feature set\n",
        "features = np.array(kronodroid_data.iloc[:, range(0, Drebin_data.shape[1]-1)])\n",
        "# labels --> B: Benign and S\n",
        "labels = kronodroid_data.iloc[:, -1]\n",
        "\n",
        "# feature scaling\n",
        "X = preprocessing.StandardScaler().fit(features).transform(features)\n",
        "\n",
        "# binarize the labels\n",
        "lb = LabelBinarizer()\n",
        "y = lb.fit_transform(labels)\n",
        "\n",
        "# split data into training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.2, random_state=100)\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(X_train.shape[1],), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(256, activation='sigmoid'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='relu'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[early_stop])\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f'Test loss: {score[0]}')\n",
        "print(f'Test accuracy: {score[1]}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Shj8uIbLl9Qw"
      },
      "source": [
        "## Autoencoder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZEcg2y5_rMx"
      },
      "source": [
        "### Drebin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qnmw75N-k758",
        "outputId": "b919fc70-95ba-4ee7-d527-3571e6873267"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "301/301 [==============================] - 3s 4ms/step - loss: 0.6592 - val_loss: 0.5092\n",
            "Epoch 2/10\n",
            "301/301 [==============================] - 1s 4ms/step - loss: 0.4508 - val_loss: 0.4057\n",
            "Epoch 3/10\n",
            "301/301 [==============================] - 1s 4ms/step - loss: 0.3701 - val_loss: 0.3585\n",
            "Epoch 4/10\n",
            "301/301 [==============================] - 1s 5ms/step - loss: 0.3213 - val_loss: 0.3290\n",
            "Epoch 5/10\n",
            "301/301 [==============================] - 2s 6ms/step - loss: 0.2933 - val_loss: 0.2946\n",
            "Epoch 6/10\n",
            "301/301 [==============================] - 2s 8ms/step - loss: 0.2663 - val_loss: 0.2770\n",
            "Epoch 7/10\n",
            "301/301 [==============================] - 2s 6ms/step - loss: 0.2470 - val_loss: 0.2854\n",
            "Epoch 8/10\n",
            "301/301 [==============================] - 2s 6ms/step - loss: 0.2387 - val_loss: 0.2539\n",
            "Epoch 9/10\n",
            "301/301 [==============================] - 2s 6ms/step - loss: 0.2243 - val_loss: 0.2471\n",
            "Epoch 10/10\n",
            "301/301 [==============================] - 2s 6ms/step - loss: 0.2134 - val_loss: 0.2364\n",
            "Test loss: 0.23934967815876007\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Load the Drebin dataset\n",
        "# drebin_df = pd.read_csv('/content/drive/MyDrive/drebin.csv')\n",
        "\n",
        "# feature set\n",
        "features = np.array(Drebin_data.iloc[:, range(0, Drebin_data.shape[1]-1)])\n",
        "# labels --> B: Benign and S\n",
        "labels = Drebin_data.iloc[:, -1]\n",
        "\n",
        "# feature scaling\n",
        "X = preprocessing.StandardScaler().fit(features).transform(features)\n",
        "\n",
        "# split data into training and test set\n",
        "X_train, X_test, _, _ = train_test_split(X, labels, shuffle=True, test_size=0.2, random_state=100)\n",
        "\n",
        "# Define the model architecture\n",
        "inputs = Input(shape=(X_train.shape[1],))\n",
        "encoded = Dense(256, activation='relu')(inputs)\n",
        "encoded = Dense(128, activation='relu')(encoded)\n",
        "encoded = Dense(64, activation='relu')(encoded)\n",
        "decoded = Dense(128, activation='relu')(encoded)\n",
        "decoded = Dense(256, activation='relu')(decoded)\n",
        "decoded = Dense(X_train.shape[1], activation='linear')(decoded)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=decoded)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, X_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[early_stop])\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "score = model.evaluate(X_test, X_test, verbose=0)\n",
        "print(f'Test loss: {score}')\n",
        "# print(f'Test accuracy: {score[1]}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRD7cOHH_yI8"
      },
      "source": [
        "### Malgenome"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLNDlRFy_xnj",
        "outputId": "7b38d736-7596-4755-e8bf-c3896a4c0880"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "76/76 [==============================] - 3s 6ms/step - loss: 0.8106 - val_loss: 0.6744\n",
            "Epoch 2/10\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.5891 - val_loss: 0.5711\n",
            "Epoch 3/10\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.4896 - val_loss: 0.5174\n",
            "Epoch 4/10\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 0.4255 - val_loss: 0.4817\n",
            "Epoch 5/10\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 0.3858 - val_loss: 0.4483\n",
            "Epoch 6/10\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.3519 - val_loss: 0.4288\n",
            "Epoch 7/10\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 0.3259 - val_loss: 0.4107\n",
            "Epoch 8/10\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.3072 - val_loss: 0.3953\n",
            "Epoch 9/10\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.2894 - val_loss: 0.3831\n",
            "Epoch 10/10\n",
            "76/76 [==============================] - 0s 7ms/step - loss: 0.2732 - val_loss: 0.3730\n",
            "Test loss: 0.40093499422073364\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Load the Drebin dataset\n",
        "Malgenome_df = pd.read_csv('/content/drive/MyDrive/malgenome.csv')\n",
        "\n",
        "# feature set\n",
        "features = np.array(Malgenome_data.iloc[:, range(0, Drebin_data.shape[1]-1)])\n",
        "# labels --> B: Benign and S\n",
        "labels = Malgenome_data.iloc[:, -1]\n",
        "\n",
        "# feature scaling\n",
        "X = preprocessing.StandardScaler().fit(features).transform(features)\n",
        "\n",
        "# split data into training and test set\n",
        "X_train, X_test, _, _ = train_test_split(X, labels, shuffle=True, test_size=0.2, random_state=100)\n",
        "\n",
        "# Define the model architecture\n",
        "inputs = Input(shape=(X_train.shape[1],))\n",
        "encoded = Dense(256, activation='relu')(inputs)\n",
        "encoded = Dense(128, activation='relu')(encoded)\n",
        "encoded = Dense(64, activation='relu')(encoded)\n",
        "decoded = Dense(128, activation='relu')(encoded)\n",
        "decoded = Dense(256, activation='relu')(decoded)\n",
        "decoded = Dense(X_train.shape[1], activation='linear')(decoded)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=decoded)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, X_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[early_stop])\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "score = model.evaluate(X_test, X_test, verbose=0)\n",
        "print(f'Test loss: {score}')\n",
        "# print(f'Test accuracy: {score[1]}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cft4fS40ANIJ"
      },
      "source": [
        "### Tuandromd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNskL_eqASc6",
        "outputId": "d2777e12-6842-4336-defe-cd43ee90475a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "90/90 [==============================] - 2s 7ms/step - loss: 0.4399 - val_loss: 0.6619\n",
            "Epoch 2/10\n",
            "90/90 [==============================] - 1s 7ms/step - loss: 0.2792 - val_loss: 0.2827\n",
            "Epoch 3/10\n",
            "90/90 [==============================] - 1s 6ms/step - loss: 0.1276 - val_loss: 0.2631\n",
            "Epoch 4/10\n",
            "90/90 [==============================] - 1s 7ms/step - loss: 0.1131 - val_loss: 0.2521\n",
            "Epoch 5/10\n",
            "90/90 [==============================] - 1s 6ms/step - loss: 0.1117 - val_loss: 0.2538\n",
            "Epoch 6/10\n",
            "90/90 [==============================] - 1s 6ms/step - loss: 0.1198 - val_loss: 0.3847\n",
            "Epoch 7/10\n",
            "90/90 [==============================] - 1s 6ms/step - loss: 0.1229 - val_loss: 0.2330\n",
            "Epoch 8/10\n",
            "90/90 [==============================] - 0s 5ms/step - loss: 0.0718 - val_loss: 0.2308\n",
            "Epoch 9/10\n",
            "90/90 [==============================] - 0s 5ms/step - loss: 0.0655 - val_loss: 0.2160\n",
            "Epoch 10/10\n",
            "90/90 [==============================] - 0s 4ms/step - loss: 0.0607 - val_loss: 0.2091\n",
            "Test loss: 0.052182331681251526\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Load the Drebin dataset\n",
        "TUANDROMD_df = pd.read_csv('/content/drive/MyDrive/TUANDROMD.csv')\n",
        "\n",
        "# feature set\n",
        "features = np.array(Tuandromd_data.iloc[:, range(0, Drebin_data.shape[1]-1)])\n",
        "# labels --> B: Benign and S\n",
        "labels = Tuandromd_data.iloc[:, -1]\n",
        "\n",
        "# feature scaling\n",
        "X = preprocessing.StandardScaler().fit(features).transform(features)\n",
        "\n",
        "# split data into training and test set\n",
        "X_train, X_test, _, _ = train_test_split(X, labels, shuffle=True, test_size=0.2, random_state=100)\n",
        "\n",
        "# Define the model architecture\n",
        "inputs = Input(shape=(X_train.shape[1],))\n",
        "encoded = Dense(256, activation='relu')(inputs)\n",
        "encoded = Dense(128, activation='relu')(encoded)\n",
        "encoded = Dense(64, activation='relu')(encoded)\n",
        "decoded = Dense(128, activation='relu')(encoded)\n",
        "decoded = Dense(256, activation='relu')(decoded)\n",
        "decoded = Dense(X_train.shape[1], activation='linear')(decoded)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=decoded)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, X_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[early_stop])\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "score = model.evaluate(X_test, X_test, verbose=0)\n",
        "print(f'Test loss: {score}')\n",
        "# print(f'Test accuracy: {score[1]}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2CBGA_ZAadY"
      },
      "source": [
        "### Kronodroid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TFZEOZxAeAp",
        "outputId": "6bcb1417-0641-4cdc-fd05-a174bf910b33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1563/1563 [==============================] - 10s 5ms/step - loss: 0.3712 - val_loss: 0.2595\n",
            "Epoch 2/10\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.3038 - val_loss: 0.2428\n",
            "Epoch 3/10\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.2915 - val_loss: 0.2399\n",
            "Epoch 4/10\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.2748 - val_loss: 0.2030\n",
            "Epoch 5/10\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.2509 - val_loss: 0.2127\n",
            "Epoch 6/10\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.2501 - val_loss: 0.1952\n",
            "Epoch 7/10\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.2450 - val_loss: 0.1888\n",
            "Epoch 8/10\n",
            "1563/1563 [==============================] - 12s 7ms/step - loss: 0.2134 - val_loss: 0.1693\n",
            "Epoch 9/10\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.2079 - val_loss: 0.1598\n",
            "Epoch 10/10\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.1961 - val_loss: 0.1710\n",
            "Test loss: 0.13036476075649261\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Load the Drebin dataset\n",
        "kronodroid_df = pd.read_csv('/content/drive/MyDrive/kronodroid.csv')\n",
        "\n",
        "# feature set\n",
        "features = np.array(kronodroid_data.iloc[:, range(0, Drebin_data.shape[1]-1)])\n",
        "# labels --> B: Benign and S\n",
        "labels = kronodroid_data.iloc[:, -1]\n",
        "\n",
        "# feature scaling\n",
        "X = preprocessing.StandardScaler().fit(features).transform(features)\n",
        "\n",
        "# split data into training and test set\n",
        "X_train, X_test, _, _ = train_test_split(X, labels, shuffle=True, test_size=0.2, random_state=100)\n",
        "\n",
        "# Define the model architecture\n",
        "inputs = Input(shape=(X_train.shape[1],))\n",
        "encoded = Dense(256, activation='relu')(inputs)\n",
        "encoded = Dense(128, activation='relu')(encoded)\n",
        "encoded = Dense(64, activation='relu')(encoded)\n",
        "decoded = Dense(128, activation='relu')(encoded)\n",
        "decoded = Dense(256, activation='relu')(decoded)\n",
        "decoded = Dense(X_train.shape[1], activation='linear')(decoded)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=decoded)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, X_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[early_stop])\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "score = model.evaluate(X_test, X_test, verbose=0)\n",
        "print(f'Test loss: {score}')\n",
        "# print(f'Test accuracy: {score[1]}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5pRrRadsnwG"
      },
      "source": [
        "## Feed Forward Neural Network Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVUYJkvvAwuh"
      },
      "source": [
        "### Drebin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svrkFwVzoQ7j",
        "outputId": "aaa73f91-9363-47d7-e53c-ec814b0e4321"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "301/301 [==============================] - 4s 7ms/step - loss: 1.0964 - accuracy: 0.8960 - val_loss: 0.6633 - val_accuracy: 0.9435\n",
            "Epoch 2/10\n",
            "301/301 [==============================] - 2s 5ms/step - loss: 0.8360 - accuracy: 0.9281 - val_loss: 0.7254 - val_accuracy: 0.9414\n",
            "Epoch 3/10\n",
            "301/301 [==============================] - 2s 5ms/step - loss: 0.8374 - accuracy: 0.9293 - val_loss: 0.5765 - val_accuracy: 0.9526\n",
            "Epoch 4/10\n",
            "301/301 [==============================] - 2s 5ms/step - loss: 0.8535 - accuracy: 0.9295 - val_loss: 0.4419 - val_accuracy: 0.9605\n",
            "Epoch 5/10\n",
            "301/301 [==============================] - 2s 7ms/step - loss: 0.6679 - accuracy: 0.9471 - val_loss: 0.4532 - val_accuracy: 0.9609\n",
            "Epoch 6/10\n",
            "301/301 [==============================] - 2s 7ms/step - loss: 0.6183 - accuracy: 0.9490 - val_loss: 0.5169 - val_accuracy: 0.9568\n",
            "Epoch 7/10\n",
            "301/301 [==============================] - 2s 5ms/step - loss: 0.6454 - accuracy: 0.9479 - val_loss: 1.4781 - val_accuracy: 0.8894\n",
            "Epoch 7: early stopping\n",
            "Test loss: 1.3338240385055542\n",
            "Test accuracy: 0.9029255509376526\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Dropout, BatchNormalization, Activation\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Load the Drebin dataset\n",
        "# drebin_df = pd.read_csv('/content/drive/MyDrive/drebin.csv')\n",
        "\n",
        "# feature set\n",
        "features = np.array(Drebin_data.iloc[:, range(0, Drebin_data.shape[1]-1)])\n",
        "# labels --> B: Benign and S\n",
        "labels = Drebin_data.iloc[:, -1]\n",
        "\n",
        "# feature scaling\n",
        "X = preprocessing.StandardScaler().fit(features).transform(features)\n",
        "\n",
        "# binarize the labels\n",
        "lb = LabelBinarizer()\n",
        "y = lb.fit_transform(labels)\n",
        "\n",
        "# split data into training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.2, random_state=100)\n",
        "\n",
        "# Define the model architecture\n",
        "inputs = Input(shape=(X_train.shape[1],))\n",
        "x = Dense(512)(inputs)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(256)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "outputs = Dense(1, activation='relu')(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[early_stop])\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f'Test loss: {score[0]}')\n",
        "print(f'Test accuracy: {score[1]}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFYDetTSA1Gk"
      },
      "source": [
        "### Malgenome"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ec4D_IziA4Ip",
        "outputId": "57e00668-b6e7-4c28-fb68-43c4806887a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "76/76 [==============================] - 4s 8ms/step - loss: 2.9300 - accuracy: 0.7701 - val_loss: 0.2969 - val_accuracy: 0.9589\n",
            "Epoch 2/10\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.6525 - accuracy: 0.9342 - val_loss: 0.3512 - val_accuracy: 0.9688\n",
            "Epoch 3/10\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 0.6292 - accuracy: 0.9404 - val_loss: 0.2760 - val_accuracy: 0.9720\n",
            "Epoch 4/10\n",
            "76/76 [==============================] - 1s 8ms/step - loss: 0.6479 - accuracy: 0.9379 - val_loss: 0.2977 - val_accuracy: 0.9753\n",
            "Epoch 5/10\n",
            "76/76 [==============================] - 1s 9ms/step - loss: 0.4748 - accuracy: 0.9564 - val_loss: 0.3092 - val_accuracy: 0.9786\n",
            "Epoch 6/10\n",
            "76/76 [==============================] - 1s 9ms/step - loss: 0.4856 - accuracy: 0.9568 - val_loss: 0.3079 - val_accuracy: 0.9803\n",
            "Epoch 6: early stopping\n",
            "Test loss: 0.12067337334156036\n",
            "Test accuracy: 0.9815789461135864\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Dropout, BatchNormalization, Activation\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Load the Drebin dataset\n",
        "Malgenome_df = pd.read_csv('/content/drive/MyDrive/malgenome.csv')\n",
        "\n",
        "# feature set\n",
        "features = np.array(Malgenome_data.iloc[:, range(0, Drebin_data.shape[1]-1)])\n",
        "# labels --> B: Benign and S\n",
        "labels = Malgenome_data.iloc[:, -1]\n",
        "\n",
        "# feature scaling\n",
        "X = preprocessing.StandardScaler().fit(features).transform(features)\n",
        "\n",
        "# binarize the labels\n",
        "lb = LabelBinarizer()\n",
        "y = lb.fit_transform(labels)\n",
        "\n",
        "# split data into training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.2, random_state=100)\n",
        "\n",
        "# Define the model architecture\n",
        "inputs = Input(shape=(X_train.shape[1],))\n",
        "x = Dense(512)(inputs)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('sigmoid')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(256)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('sigmoid')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "outputs = Dense(1, activation='relu')(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[early_stop])\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f'Test loss: {score[0]}')\n",
        "print(f'Test accuracy: {score[1]}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qc8I5VPNBgSD"
      },
      "source": [
        "### Tuandromd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_OOXuVxBi7q",
        "outputId": "7ea30a39-5400-4336-8198-9628a559e095"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "90/90 [==============================] - 4s 8ms/step - loss: 1.8518 - accuracy: 0.8141 - val_loss: 1.1923 - val_accuracy: 0.8378\n",
            "Epoch 2/10\n",
            "90/90 [==============================] - 1s 6ms/step - loss: 0.9818 - accuracy: 0.9065 - val_loss: 0.4318 - val_accuracy: 0.9497\n",
            "Epoch 3/10\n",
            "90/90 [==============================] - 1s 7ms/step - loss: 0.7158 - accuracy: 0.9240 - val_loss: 0.4003 - val_accuracy: 0.9594\n",
            "Epoch 4/10\n",
            "90/90 [==============================] - 1s 7ms/step - loss: 0.8818 - accuracy: 0.9170 - val_loss: 0.4582 - val_accuracy: 0.9594\n",
            "Epoch 5/10\n",
            "90/90 [==============================] - 1s 6ms/step - loss: 0.6019 - accuracy: 0.9366 - val_loss: 0.3160 - val_accuracy: 0.9706\n",
            "Epoch 6/10\n",
            "90/90 [==============================] - 1s 6ms/step - loss: 0.5390 - accuracy: 0.9492 - val_loss: 0.3086 - val_accuracy: 0.9776\n",
            "Epoch 7/10\n",
            "90/90 [==============================] - 1s 6ms/step - loss: 0.5583 - accuracy: 0.9454 - val_loss: 0.2752 - val_accuracy: 0.9762\n",
            "Epoch 8/10\n",
            "90/90 [==============================] - 1s 6ms/step - loss: 0.6866 - accuracy: 0.9380 - val_loss: 0.3175 - val_accuracy: 0.9720\n",
            "Epoch 9/10\n",
            "90/90 [==============================] - 1s 6ms/step - loss: 0.4891 - accuracy: 0.9506 - val_loss: 0.3078 - val_accuracy: 0.9748\n",
            "Epoch 10/10\n",
            "90/90 [==============================] - 1s 6ms/step - loss: 0.5346 - accuracy: 0.9433 - val_loss: 0.2868 - val_accuracy: 0.9776\n",
            "Epoch 10: early stopping\n",
            "Test loss: 0.4539395570755005\n",
            "Test accuracy: 0.9675251841545105\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Dropout, BatchNormalization, Activation\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Load the Drebin dataset\n",
        "TUANDROMD_df = pd.read_csv('/content/drive/MyDrive/TUANDROMD.csv')\n",
        "\n",
        "# feature set\n",
        "features = np.array(Tuandromd_data.iloc[:, range(0, Drebin_data.shape[1]-1)])\n",
        "# labels --> B: Benign and S\n",
        "labels = Tuandromd_data.iloc[:, -1]\n",
        "\n",
        "# feature scaling\n",
        "X = preprocessing.StandardScaler().fit(features).transform(features)\n",
        "\n",
        "# binarize the labels\n",
        "lb = LabelBinarizer()\n",
        "y = lb.fit_transform(labels)\n",
        "\n",
        "# split data into training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.2, random_state=100)\n",
        "\n",
        "# Define the model architecture\n",
        "inputs = Input(shape=(X_train.shape[1],))\n",
        "x = Dense(512)(inputs)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('sigmoid')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(256)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('sigmoid')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "outputs = Dense(1, activation='relu')(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[early_stop])\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f'Test loss: {score[0]}')\n",
        "print(f'Test accuracy: {score[1]}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33x_x_QFBq6x"
      },
      "source": [
        "### Kronodroid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYt7i00hBuYQ",
        "outputId": "ffac9727-da85-4e2d-e317-d4f9e7b0e531"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1563/1563 [==============================] - 14s 7ms/step - loss: 1.1958 - accuracy: 0.8828 - val_loss: 0.1465 - val_accuracy: 0.9708\n",
            "Epoch 2/10\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 1.0040 - accuracy: 0.9095 - val_loss: 0.7245 - val_accuracy: 0.9131\n",
            "Epoch 3/10\n",
            "1563/1563 [==============================] - 10s 7ms/step - loss: 1.2057 - accuracy: 0.8967 - val_loss: 0.4389 - val_accuracy: 0.9473\n",
            "Epoch 4/10\n",
            "1563/1563 [==============================] - 10s 7ms/step - loss: 0.8791 - accuracy: 0.9216 - val_loss: 1.0344 - val_accuracy: 0.9134\n",
            "Epoch 4: early stopping\n",
            "Test loss: 1.0376754999160767\n",
            "Test accuracy: 0.9089455008506775\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Dropout, BatchNormalization, Activation\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Load the Drebin dataset\n",
        "kronodroid_df = pd.read_csv('/content/drive/MyDrive/kronodroid.csv')\n",
        "\n",
        "# feature set\n",
        "features = np.array(kronodroid_data.iloc[:, range(0, Drebin_data.shape[1]-1)])\n",
        "# labels --> B: Benign and S\n",
        "labels = kronodroid_data.iloc[:, -1]\n",
        "\n",
        "# feature scaling\n",
        "X = preprocessing.StandardScaler().fit(features).transform(features)\n",
        "\n",
        "# binarize the labels\n",
        "lb = LabelBinarizer()\n",
        "y = lb.fit_transform(labels)\n",
        "\n",
        "# split data into training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.2, random_state=100)\n",
        "\n",
        "# Define the model architecture\n",
        "inputs = Input(shape=(X_train.shape[1],))\n",
        "x = Dense(512)(inputs)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('sigmoid')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(256)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('sigmoid')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "outputs = Dense(1, activation='relu')(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[early_stop])\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f'Test loss: {score[0]}')\n",
        "print(f'Test accuracy: {score[1]}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hdl0RHvfzaq"
      },
      "source": [
        "## GRU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQXneEYpCS6E"
      },
      "source": [
        "### Drebin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eewazB3teuku",
        "outputId": "49468a6e-f33f-4969-9ebe-b9fd9a6f4f50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "301/301 [==============================] - 15s 13ms/step - loss: 0.6925 - accuracy: 0.6003 - val_loss: 0.6627 - val_accuracy: 0.6334\n",
            "Epoch 2/10\n",
            "301/301 [==============================] - 3s 11ms/step - loss: 0.6658 - accuracy: 0.6222 - val_loss: 0.6573 - val_accuracy: 0.6334\n",
            "Epoch 3/10\n",
            "301/301 [==============================] - 3s 12ms/step - loss: 0.6633 - accuracy: 0.6237 - val_loss: 0.6610 - val_accuracy: 0.6334\n",
            "Epoch 4/10\n",
            "301/301 [==============================] - 4s 14ms/step - loss: 0.6627 - accuracy: 0.6240 - val_loss: 0.6590 - val_accuracy: 0.6301\n",
            "Epoch 5/10\n",
            "301/301 [==============================] - 3s 11ms/step - loss: 0.6601 - accuracy: 0.6269 - val_loss: 0.6565 - val_accuracy: 0.6313\n",
            "Epoch 6/10\n",
            "301/301 [==============================] - 4s 12ms/step - loss: 0.6596 - accuracy: 0.6266 - val_loss: 0.6544 - val_accuracy: 0.6363\n",
            "Epoch 7/10\n",
            "301/301 [==============================] - 4s 13ms/step - loss: 0.6557 - accuracy: 0.6311 - val_loss: 0.6502 - val_accuracy: 0.6313\n",
            "Epoch 8/10\n",
            "301/301 [==============================] - 4s 13ms/step - loss: 0.6497 - accuracy: 0.6351 - val_loss: 0.6367 - val_accuracy: 0.6530\n",
            "Epoch 9/10\n",
            "301/301 [==============================] - 3s 11ms/step - loss: 0.6437 - accuracy: 0.6443 - val_loss: 0.6345 - val_accuracy: 0.6517\n",
            "Epoch 10/10\n",
            "301/301 [==============================] - 3s 11ms/step - loss: 0.6386 - accuracy: 0.6509 - val_loss: 0.6252 - val_accuracy: 0.6683\n",
            "Test loss: 0.6206555366516113\n",
            "Test accuracy: 0.666223406791687\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Dropout, BatchNormalization, Activation, GRU\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# Load the Drebin dataset\n",
        "drebin_df = pd.read_csv('/content/drive/MyDrive/drebin.csv')\n",
        "\n",
        "# feature set\n",
        "features = np.array(Drebin_data.iloc[:, range(0, Drebin_data.shape[1]-1)])\n",
        "# labels --> B: Benign and S\n",
        "labels = Drebin_data.iloc[:, -1]\n",
        "\n",
        "# feature scaling\n",
        "X = preprocessing.StandardScaler().fit(features).transform(features)\n",
        "\n",
        "# binarize the labels\n",
        "lb = preprocessing.LabelBinarizer()\n",
        "y = lb.fit_transform(labels)\n",
        "\n",
        "# split data into training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.2, random_state=100)\n",
        "\n",
        "# Define the model architecture\n",
        "inputs = Input(shape=(X_train.shape[1],1))\n",
        "x = GRU(64)(inputs)\n",
        "x = Dense(32)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(16)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train[:,:,np.newaxis], y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[early_stop])\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "score = model.evaluate(X_test[:,:,np.newaxis], y_test, verbose=0)\n",
        "print(f'Test loss: {score[0]}')\n",
        "print(f'Test accuracy: {score[1]}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRpySHviCX5B"
      },
      "source": [
        "### Malgenome"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woi8zzzfCZmG",
        "outputId": "a2dd568b-3004-45e2-977f-02d938576eb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "76/76 [==============================] - 6s 19ms/step - loss: 0.7926 - accuracy: 0.4299 - val_loss: 0.6679 - val_accuracy: 0.6760\n",
            "Epoch 2/10\n",
            "76/76 [==============================] - 1s 11ms/step - loss: 0.6978 - accuracy: 0.5858 - val_loss: 0.6499 - val_accuracy: 0.6760\n",
            "Epoch 3/10\n",
            "76/76 [==============================] - 1s 18ms/step - loss: 0.6701 - accuracy: 0.6384 - val_loss: 0.6447 - val_accuracy: 0.6760\n",
            "Epoch 4/10\n",
            "76/76 [==============================] - 1s 18ms/step - loss: 0.6529 - accuracy: 0.6610 - val_loss: 0.6411 - val_accuracy: 0.6760\n",
            "Epoch 5/10\n",
            "76/76 [==============================] - 1s 16ms/step - loss: 0.6448 - accuracy: 0.6643 - val_loss: 0.6486 - val_accuracy: 0.6760\n",
            "Epoch 6/10\n",
            "76/76 [==============================] - 1s 11ms/step - loss: 0.6500 - accuracy: 0.6639 - val_loss: 0.6420 - val_accuracy: 0.6743\n",
            "Epoch 7/10\n",
            "76/76 [==============================] - 1s 12ms/step - loss: 0.6418 - accuracy: 0.6656 - val_loss: 0.6349 - val_accuracy: 0.6826\n",
            "Epoch 8/10\n",
            "76/76 [==============================] - 1s 11ms/step - loss: 0.6387 - accuracy: 0.6672 - val_loss: 0.6375 - val_accuracy: 0.6776\n",
            "Epoch 9/10\n",
            "76/76 [==============================] - 1s 11ms/step - loss: 0.6425 - accuracy: 0.6672 - val_loss: 0.6384 - val_accuracy: 0.6809\n",
            "Epoch 10/10\n",
            "76/76 [==============================] - 1s 13ms/step - loss: 0.6391 - accuracy: 0.6672 - val_loss: 0.6377 - val_accuracy: 0.6727\n",
            "Epoch 10: early stopping\n",
            "Test loss: 0.6373704671859741\n",
            "Test accuracy: 0.6592105031013489\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Dropout, BatchNormalization, Activation, GRU\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# Load the Drebin dataset\n",
        "Malgenome_df = pd.read_csv('/content/drive/MyDrive/malgenome.csv')\n",
        "\n",
        "# feature set\n",
        "features = np.array(Malgenome_data.iloc[:, range(0, Drebin_data.shape[1]-1)])\n",
        "# labels --> B: Benign and S\n",
        "labels = Malgenome_data.iloc[:, -1]\n",
        "\n",
        "# feature scaling\n",
        "X = preprocessing.StandardScaler().fit(features).transform(features)\n",
        "\n",
        "# binarize the labels\n",
        "lb = preprocessing.LabelBinarizer()\n",
        "y = lb.fit_transform(labels)\n",
        "\n",
        "# split data into training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.2, random_state=100)\n",
        "\n",
        "# Define the model architecture\n",
        "inputs = Input(shape=(X_train.shape[1],1))\n",
        "x = GRU(64)(inputs)\n",
        "x = Dense(32)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(16)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train[:,:,np.newaxis], y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[early_stop])\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "score = model.evaluate(X_test[:,:,np.newaxis], y_test, verbose=0)\n",
        "print(f'Test loss: {score[0]}')\n",
        "print(f'Test accuracy: {score[1]}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkkj_8U8CjZl"
      },
      "source": [
        "### Tuandromd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWS6Q4nvClla",
        "outputId": "47939239-dfaa-40c4-f7e3-20ddce5067aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "90/90 [==============================] - 5s 17ms/step - loss: 0.5536 - accuracy: 0.7563 - val_loss: 0.5925 - val_accuracy: 0.7972\n",
            "Epoch 2/10\n",
            "90/90 [==============================] - 2s 20ms/step - loss: 0.5148 - accuracy: 0.7906 - val_loss: 0.5538 - val_accuracy: 0.7972\n",
            "Epoch 3/10\n",
            "90/90 [==============================] - 2s 24ms/step - loss: 0.5021 - accuracy: 0.7959 - val_loss: 0.5263 - val_accuracy: 0.7972\n",
            "Epoch 4/10\n",
            "90/90 [==============================] - 2s 24ms/step - loss: 0.5007 - accuracy: 0.7938 - val_loss: 0.5090 - val_accuracy: 0.7972\n",
            "Epoch 5/10\n",
            "90/90 [==============================] - 1s 15ms/step - loss: 0.4970 - accuracy: 0.7948 - val_loss: 0.4901 - val_accuracy: 0.7972\n",
            "Epoch 6/10\n",
            "90/90 [==============================] - 1s 12ms/step - loss: 0.4933 - accuracy: 0.8011 - val_loss: 0.4799 - val_accuracy: 0.8070\n",
            "Epoch 7/10\n",
            "90/90 [==============================] - 1s 13ms/step - loss: 0.4892 - accuracy: 0.8011 - val_loss: 0.4773 - val_accuracy: 0.8112\n",
            "Epoch 8/10\n",
            "90/90 [==============================] - 1s 13ms/step - loss: 0.4819 - accuracy: 0.8043 - val_loss: 0.4649 - val_accuracy: 0.8140\n",
            "Epoch 9/10\n",
            "90/90 [==============================] - 1s 12ms/step - loss: 0.4570 - accuracy: 0.8039 - val_loss: 0.8437 - val_accuracy: 0.3049\n",
            "Epoch 10/10\n",
            "90/90 [==============================] - 1s 12ms/step - loss: 0.3350 - accuracy: 0.8554 - val_loss: 0.8487 - val_accuracy: 0.8210\n",
            "Test loss: 0.8650866150856018\n",
            "Test accuracy: 0.8230683207511902\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Dropout, BatchNormalization, Activation, GRU\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# Load the Drebin dataset\n",
        "TUANDROMD_df = pd.read_csv('/content/drive/MyDrive/TUANDROMD.csv')\n",
        "\n",
        "# feature set\n",
        "features = np.array(Tuandromd_data.iloc[:, range(0, Drebin_data.shape[1]-1)])\n",
        "# labels --> B: Benign and S\n",
        "labels = Tuandromd_data.iloc[:, -1]\n",
        "\n",
        "# feature scaling\n",
        "X = preprocessing.StandardScaler().fit(features).transform(features)\n",
        "\n",
        "# binarize the labels\n",
        "lb = preprocessing.LabelBinarizer()\n",
        "y = lb.fit_transform(labels)\n",
        "\n",
        "# split data into training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.2, random_state=100)\n",
        "\n",
        "# Define the model architecture\n",
        "inputs = Input(shape=(X_train.shape[1],1))\n",
        "x = GRU(64)(inputs)\n",
        "x = Dense(32)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(16)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train[:,:,np.newaxis], y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[early_stop])\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "score = model.evaluate(X_test[:,:,np.newaxis], y_test, verbose=0)\n",
        "print(f'Test loss: {score[0]}')\n",
        "print(f'Test accuracy: {score[1]}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obE2SQ4_C3GK"
      },
      "source": [
        "### Kronodroid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4wvgE56C6dR",
        "outputId": "8c694c1a-2d5c-4810-f75b-af105dcb519a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1563/1563 [==============================] - 26s 13ms/step - loss: 0.5650 - accuracy: 0.7036 - val_loss: 0.5379 - val_accuracy: 0.7208\n",
            "Epoch 2/10\n",
            "1563/1563 [==============================] - 20s 13ms/step - loss: 0.4486 - accuracy: 0.7848 - val_loss: 0.0726 - val_accuracy: 0.9774\n",
            "Epoch 3/10\n",
            "1563/1563 [==============================] - 19s 12ms/step - loss: 0.1438 - accuracy: 0.9574 - val_loss: 0.2641 - val_accuracy: 0.9111\n",
            "Epoch 4/10\n",
            "1563/1563 [==============================] - 20s 13ms/step - loss: 0.0688 - accuracy: 0.9807 - val_loss: 0.0390 - val_accuracy: 0.9886\n",
            "Epoch 5/10\n",
            "1563/1563 [==============================] - 19s 12ms/step - loss: 0.0702 - accuracy: 0.9807 - val_loss: 0.0308 - val_accuracy: 0.9915\n",
            "Epoch 6/10\n",
            "1563/1563 [==============================] - 21s 13ms/step - loss: 0.0501 - accuracy: 0.9847 - val_loss: 0.0241 - val_accuracy: 0.9903\n",
            "Epoch 7/10\n",
            "1563/1563 [==============================] - 20s 13ms/step - loss: 0.0546 - accuracy: 0.9854 - val_loss: 0.2189 - val_accuracy: 0.9357\n",
            "Epoch 8/10\n",
            "1563/1563 [==============================] - 22s 14ms/step - loss: 0.0258 - accuracy: 0.9918 - val_loss: 0.0126 - val_accuracy: 0.9959\n",
            "Epoch 9/10\n",
            "1563/1563 [==============================] - 19s 12ms/step - loss: 0.0558 - accuracy: 0.9853 - val_loss: 0.1014 - val_accuracy: 0.9660\n",
            "Epoch 10/10\n",
            "1563/1563 [==============================] - 20s 13ms/step - loss: 0.0385 - accuracy: 0.9897 - val_loss: 1.6575 - val_accuracy: 0.5358\n",
            "Test loss: 1.6884340047836304\n",
            "Test accuracy: 0.5326977372169495\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Dropout, BatchNormalization, Activation, GRU\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# Load the Drebin dataset\n",
        "kronodroid_df = pd.read_csv('/content/drive/MyDrive/kronodroid.csv')\n",
        "\n",
        "# feature set\n",
        "features = np.array(kronodroid_data.iloc[:, range(0, Drebin_data.shape[1]-1)])\n",
        "# labels --> B: Benign and S\n",
        "labels = kronodroid_data.iloc[:, -1]\n",
        "\n",
        "# feature scaling\n",
        "X = preprocessing.StandardScaler().fit(features).transform(features)\n",
        "\n",
        "# binarize the labels\n",
        "lb = preprocessing.LabelBinarizer()\n",
        "y = lb.fit_transform(labels)\n",
        "\n",
        "# split data into training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.2, random_state=100)\n",
        "\n",
        "# Define the model architecture\n",
        "inputs = Input(shape=(X_train.shape[1],1))\n",
        "x = GRU(64)(inputs)\n",
        "x = Dense(32)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(16)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train[:,:,np.newaxis], y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[early_stop])\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "score = model.evaluate(X_test[:,:,np.newaxis], y_test, verbose=0)\n",
        "print(f'Test loss: {score[0]}')\n",
        "print(f'Test accuracy: {score[1]}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6EsyuSXhmSP"
      },
      "source": [
        "# BiLSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihj5Vxfwh89r",
        "outputId": "757dd721-f404-4484-cd14-1e55da9f375e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "301/301 [==============================] - 20s 40ms/step - loss: 0.3850 - accuracy: 0.3707 - val_loss: 0.3346 - val_accuracy: 0.3666\n",
            "Epoch 2/10\n",
            "301/301 [==============================] - 10s 34ms/step - loss: 0.3231 - accuracy: 0.3707 - val_loss: 0.3102 - val_accuracy: 0.3666\n",
            "Epoch 3/10\n",
            "301/301 [==============================] - 10s 32ms/step - loss: 0.2978 - accuracy: 0.3707 - val_loss: 0.2824 - val_accuracy: 0.3666\n",
            "Epoch 4/10\n",
            "301/301 [==============================] - 12s 38ms/step - loss: 0.2827 - accuracy: 0.3707 - val_loss: 0.2708 - val_accuracy: 0.3666\n",
            "Epoch 5/10\n",
            "301/301 [==============================] - 10s 34ms/step - loss: 0.2610 - accuracy: 0.3707 - val_loss: 0.2525 - val_accuracy: 0.3666\n",
            "Epoch 6/10\n",
            "301/301 [==============================] - 10s 34ms/step - loss: 0.2249 - accuracy: 0.3707 - val_loss: 0.2121 - val_accuracy: 0.3666\n",
            "Epoch 7/10\n",
            "301/301 [==============================] - 10s 33ms/step - loss: 0.2164 - accuracy: 0.3707 - val_loss: 0.1971 - val_accuracy: 0.3666\n",
            "Epoch 8/10\n",
            "301/301 [==============================] - 10s 33ms/step - loss: 0.1870 - accuracy: 0.3707 - val_loss: 0.1556 - val_accuracy: 0.3666\n",
            "Epoch 9/10\n",
            "301/301 [==============================] - 11s 36ms/step - loss: 0.1707 - accuracy: 0.3707 - val_loss: 0.1515 - val_accuracy: 0.3666\n",
            "Epoch 10/10\n",
            "301/301 [==============================] - 10s 34ms/step - loss: 0.1703 - accuracy: 0.3707 - val_loss: 0.1600 - val_accuracy: 0.3666\n",
            "Test loss: 0.1640653908252716\n",
            "Test accuracy: 0.3691386878490448\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelBinarizer, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Bidirectional, LSTM, Dense, Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Load the Drebin dataset\n",
        "Drebin_data = pd.read_csv('/content/drive/MyDrive/drebin.csv')\n",
        "\n",
        "# feature set\n",
        "features = np.array(Drebin_data.iloc[:, range(0, Drebin_data.shape[1]-1)])\n",
        "# labels --> B: Benign and S\n",
        "labels = Drebin_data.iloc[:, -1]\n",
        "\n",
        "# feature scaling\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(features)\n",
        "\n",
        "# binarize the labels\n",
        "lb = LabelBinarizer()\n",
        "y = lb.fit_transform(labels)\n",
        "\n",
        "# split data into training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.2, random_state=100)\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=(X_train.shape[1], 1)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Bidirectional(LSTM(64)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
        "\n",
        "# Train the model\n",
        "model.fit(np.expand_dims(X_train, axis=-1), y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[early_stop])\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "score = model.evaluate(np.expand_dims(X_test, axis=-1), y_test, verbose=0)\n",
        "print(f'Test loss: {score[0]}')\n",
        "print(f'Test accuracy: {score[1]}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "em3sIUr2j9J_",
        "outputId": "b6c2c401-455f-4c76-bb1d-cda4e2da250a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "255/301 [========================>.....] - ETA: 18s - loss: 0.7355 - accuracy: 0.3667"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-9718a5f6ecce>\u001b[0m in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m#Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m#Evaluate the model on the testing set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelBinarizer, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import SimpleRNN, Dense, Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "#Load the Drebin dataset\n",
        "Drebin_data = pd.read_csv('/content/drive/MyDrive/drebin.csv')\n",
        "\n",
        "#feature set\n",
        "features = np.array(Drebin_data.iloc[:, range(0, Drebin_data.shape[1]-1)])\n",
        "\n",
        "#labels --> B: Benign and S\n",
        "labels = Drebin_data.iloc[:, -1]\n",
        "\n",
        "#feature scaling\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(features)\n",
        "\n",
        "#binarize the labels\n",
        "lb = LabelBinarizer()\n",
        "y = lb.fit_transform(labels)\n",
        "\n",
        "#split data into training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.2, random_state=100)\n",
        "\n",
        "#Define the model architecture\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(128, input_shape=(X_train.shape[1], 1), return_sequences=True))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(SimpleRNN(64))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
        "\n",
        "#Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "#Define early stopping callback\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
        "\n",
        "#Train the model\n",
        "model.fit(np.expand_dims(X_train, axis=-1), y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[early_stop])\n",
        "\n",
        "#Evaluate the model on the testing set\n",
        "score = model.evaluate(np.expand_dims(X_test, axis=-1), y_test, verbose=0)\n",
        "print(f'Test loss: {score[0]}')\n",
        "print(f'Test accuracy: {score[1]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjD1QQpl2AG9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4T4aoRGk2RJ"
      },
      "source": [
        "# RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_d93hrcokhsq",
        "outputId": "0a411c10-e265-4651-eafd-1e94e64a4285"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "301/301 [==============================] - 13s 20ms/step - loss: 0.6621 - accuracy: 0.3707 - val_loss: 0.6564 - val_accuracy: 0.3666\n",
            "Epoch 2/10\n",
            "301/301 [==============================] - 5s 17ms/step - loss: 0.6554 - accuracy: 0.3707 - val_loss: 0.6571 - val_accuracy: 0.3666\n",
            "Epoch 3/10\n",
            "301/301 [==============================] - 6s 19ms/step - loss: 0.6617 - accuracy: 0.3707 - val_loss: 0.6572 - val_accuracy: 0.3666\n",
            "Epoch 4/10\n",
            "301/301 [==============================] - 5s 16ms/step - loss: 0.6616 - accuracy: 0.3707 - val_loss: 0.6575 - val_accuracy: 0.3666\n",
            "Epoch 4: early stopping\n",
            "Test loss: 0.6587600708007812\n",
            "Test accuracy: 0.3691386878490448\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelBinarizer, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Load the Drebin dataset\n",
        "Drebin_data = pd.read_csv('/content/drive/MyDrive/drebin.csv')\n",
        "\n",
        "# feature set\n",
        "features = np.array(Drebin_data.iloc[:, range(0, Drebin_data.shape[1]-1)])\n",
        "# labels --> B: Benign and S\n",
        "labels = Drebin_data.iloc[:, -1]\n",
        "\n",
        "# feature scaling\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(features)\n",
        "\n",
        "# binarize the labels\n",
        "lb = LabelBinarizer()\n",
        "y = lb.fit_transform(labels)\n",
        "\n",
        "# split data into training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.2, random_state=100)\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(LSTM(64))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
        "\n",
        "# Train the model\n",
        "model.fit(np.expand_dims(X_train, axis=-1), y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[early_stop])\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "score = model.evaluate(np.expand_dims(X_test, axis=-1), y_test, verbose=0)\n",
        "print(f'Test loss: {score[0]}')\n",
        "print(f'Test accuracy: {score[1]}')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}